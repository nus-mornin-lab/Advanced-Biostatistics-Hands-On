{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Models\n",
    "\n",
    "Objectives:\n",
    "1.  Visualize the iris dataset\n",
    "2.  Build decision trees to fit the iris dataset\n",
    "3.  Visualize decision trees\n",
    "4.  Build a random forest and an Adaboost model to fit the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly==5.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the iris dataset for this hands-on session.\n",
    "\n",
    "The iris dataset consists of 3 different types of irises' (Setosa, Versicolour, and Virginica) petal and sepal length. There are 50 instances for each class.\n",
    "\n",
    "![](Figs/iris-machinelearning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the iris dataset\n",
    "df = px.data.iris()\n",
    "x_all = df[[\"sepal_length\",\"sepal_width\", \"petal_length\", \"petal_width\"]].to_numpy()\n",
    "y_all = df[\"species_id\"].to_numpy()-1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x='sepal_length', y='petal_length', z='petal_width',\n",
    "              color='species', opacity=0.7)\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "We will build decision trees by calling build-in methods in the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we do a train-test split.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "id_all = np.arange(len(df))\n",
    "x, x_test, y, y_test, id_train, id_test = train_test_split(x_all,y_all,id_all, test_size=0.33, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also split the dataframe accordingly for later illustrations.\n",
    "\n",
    "df_train = df.iloc[id_train]\n",
    "df_test = df.iloc[id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# First we fit a shallow tree.\n",
    "TreeClassifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2, random_state=15)\n",
    "clf = TreeClassifier.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple decision tree can be easily visualized.\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(14, 10))\n",
    "plot_tree(clf, filled=True,\n",
    "          feature_names=[\"sepal_length\",\"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "          class_names=[\"Setosa\", \"Versicolour\", \"Virginica\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that a decision tree make splittings on feature variables to make predictions.\n",
    "# Let us visualize how the feature space is divided.\n",
    "\n",
    "fig = px.scatter_3d(df_train, x='sepal_length', z='petal_length', y='petal_width',\n",
    "                    opacity=0.7, color='species',\n",
    "                    color_discrete_map={\n",
    "                         \"setosa\": \"orange\",\n",
    "                         \"versicolor\": \"green\",\n",
    "                         \"virginica\": \"purple\"}\n",
    "                    )\n",
    "fig.update_traces(marker_size=5)\n",
    "X = [df['sepal_length'].min(), df['sepal_length'].max()]\n",
    "Y = [df['petal_width'].min(), df['petal_width'].max()]\n",
    "Z = [[2.45, 2.45],\n",
    "     [2.45, 2.45]]\n",
    "fig.add_trace(go.Surface(x=X,y=Y,z=Z, opacity=0.2, showscale=False))\n",
    "Z2 = [[4.75, 4.75],\n",
    "     [4.75, 4.75]]\n",
    "fig.add_trace(go.Surface(x=X,y=Y,z=Z2, opacity=0.2, showscale=False))\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also visualize on the test dataset.\n",
    "\n",
    "fig = px.scatter_3d(df_test, x='sepal_length', z='petal_length', y='petal_width',\n",
    "                    opacity=0.7, color='species',\n",
    "                    color_discrete_map={\n",
    "                         \"setosa\": \"orange\",\n",
    "                         \"versicolor\": \"green\",\n",
    "                         \"virginica\": \"purple\"}\n",
    "                    )\n",
    "fig.update_traces(marker_size=5)\n",
    "X = [df['sepal_length'].min(), df['sepal_length'].max()]\n",
    "Y = [df['petal_width'].min(), df['petal_width'].max()]\n",
    "Z = [[2.45, 2.45],\n",
    "     [2.45, 2.45]]\n",
    "fig.add_trace(go.Surface(x=X,y=Y,z=Z, opacity=0.2, showscale=False))\n",
    "Z2 = [[4.75, 4.75],\n",
    "     [4.75, 4.75]]\n",
    "fig.add_trace(go.Surface(x=X,y=Y,z=Z2, opacity=0.2, showscale=False))\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us test performence on test set.\n",
    "\n",
    "y_test_pred = clf.predict(x_test)\n",
    "print((y_test==y_test_pred).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we fit a deeper tree.\n",
    "TreeClassifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=15)\n",
    "clf_deep = TreeClassifier.fit(x,y)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plot_tree(clf_deep, filled=True,\n",
    "          feature_names=[\"sepal_length\",\"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "          class_names=[\"Setosa\", \"Versicolour\", \"Virginica\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us test performence on test set.\n",
    "\n",
    "y_test_pred = clf_deep.predict(x_test)\n",
    "print((y_test==y_test_pred).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We see that a single decision tree can achieve $92\\%$ accuracy on our test dataset. Increasing the tree size can fit the training data perfectly, but performance on test dataset may drop, which indicates overfitting.\n",
    "\n",
    "In this section, we test performance of a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ForestClassifier =RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    n_jobs=-1, # Use all CPU cores to fit trees\n",
    "    max_samples = 0.8,\n",
    "    criterion='entropy'\n",
    ")\n",
    "rfc = ForestClassifier.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest model contain many decision trees, and the final prediction is based on majority voting. This makes visualization very difficult. One way is to just visualize individual trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = rfc.estimators_\n",
    "tree1 = trees[0]\n",
    "plt.figure(figsize=(14, 10))\n",
    "plot_tree(tree1, filled=True,\n",
    "          feature_names=[\"sepal_length\",\"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "          class_names=[\"Setosa\", \"Versicolour\", \"Virginica\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rfc.predict(x_test)\n",
    "print((y_test==y_test_pred).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please tune the parameters in the random forest model to see how it affects the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Lastly let us try the AdaBoost method. Fitting the model is almost identical to fitting a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "BoostClassifier = AdaBoostClassifier(n_estimators=50)\n",
    "abc = BoostClassifier.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the trees in AdaBoost have different roles, we cannot visualize individual trees. Let us test model performance directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = abc.predict(x_test)\n",
    "print((y_test==y_test_pred).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not see performance difference between random forest and AdaBoost. This is because our dataset is small and easy. You can try these two methods on more complicated dataset!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e84f0b0af5ca574cc936389bd8ce930b03b6ba66c346a207ba44665da503187e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
