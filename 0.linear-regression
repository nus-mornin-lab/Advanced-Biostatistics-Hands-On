{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAUE4hqdgV5W"
   },
   "source": [
    "# Lecture2: Hands On Regression Analysis\n",
    "\n",
    "scopes:\n",
    "  * Basic concepts of Regression Analysis \n",
    "  * Mathematical definition of Regression Analysis \n",
    "  * Derivation of Cost function\n",
    "  * Different Minimization methods\n",
    "  * Applying the above techniques to the [US housing data](https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv) using [Pytorch](https://pytorch.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8m_AfeNYBf6"
   },
   "source": [
    "# Regression Analysis: Intuition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHjxIDGhYPy-"
   },
   "source": [
    "Linear Regression (LR) is one of the simplest AI model for predicting continuous outcomes (e.g. time, length). As shown in the figure below, LR tries to learn the statistics (patterns) that exist in a dataset by trying to draw the best fist linear line to the data.\n",
    "\n",
    "For instance, by defining the X-axis and Y-axis in the figure as weight and height respectively, we can represent the weight and height information of an $i$th indivial as a point $(x^{(i)}, y^{(i)})$. By finding the best fitting line on the cloud of points, the LR model can understand the underlying pattern between the two variables (weigth & height). If the datapoints are unbiaesd representatives of the true population we are interested in, the fitted (trained) model will have a power to an predict an individual's height using the person's weight.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"75%\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\"> \n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uljU2rTZ5EQ"
   },
   "source": [
    "# Regression Analysis: definition\n",
    "\n",
    "The goal of LR is to predict a target value $y \\in \\mathbb{R}$ from an input vector $\\mathbf{x} \\in \\mathbb{R}^m$. where $m$ is number of features. For the example introduced earlier, $y$ and $\\mathbf{x}$ represents the height and weight respectively with $k = 1$. \n",
    "\n",
    "Our goal is to find the **true** function $f_{\\theta} : \\mathbf{x} \\mapsto y$ that correctly maps an input variable to its correct targer variable. However, having only a subset of the entire popultation data with sample size of $N$ (i.e. $\\{{(\\mathbf{x},y)}\\}_{i = 1}^{N}$), we can only attain an approximated version of the function $\\hat{f_{\\theta}}$ . With more datapoints, we will have a closer estimation of the true association at a population leve. If we succeed in finding the function, we merely hope that the function can generalize over other unseeen subset of data in future.\n",
    "\n",
    "## Inference\n",
    "\n",
    "So how does the function $f_{\\theta}$ looks like? In LR, we greatly constrain the flexibilty of the function $f_{\\theta}$ by only allowing a linear association. Thus we define LR as a dot product between an input variable $\\mathbf{x}_i$ and a weight (paramter) vector $\\mathbf{\\theta} \\in \\mathbb{R}^{1\\times M} $ :\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{x}^{(i)}) = \\theta_{0}x_{0} + \\theta_{1}x_{1} + ... + \\theta_{M}x_{M} = \\sum_{k = 0}^{M}{ \\theta_{k} x_{k}^{(i)} }\n",
    "\\end{equation}\n",
    "\n",
    "Where $x_0$ is usually set to 1 to control the model bias.\n",
    "\n",
    "## {Loss, Cost, Objective} function\n",
    "\n",
    "With the above representation of $f_{\\theta}$, our task is to find a parameter vector $\\mathbf{\\theta} $ that minimizes the Mean Square Error $ J(\\theta) $ between $f_{\\theta}(\\mathbf{x})$ and $y$ :\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\frac{1}{2N} \\sum_{i = 0}^{N} \\left( f_\\theta(\\mathbf{x}^{(i)}) - y^{(i)} \\right)^2 \n",
    "\\end{equation}\n",
    "\n",
    "This function $J(\\theta)$ is called a **loss function**.\n",
    "\n",
    "## To matrix form\n",
    "\n",
    "It is often conventient to re-respresent math equations that deals with large amount of data as concise matrix operations. Therefore, the euations above can be re-written as :\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{X}) = \\mathbf{X}\\mathbf{\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\frac{1}{2N} (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})^T(\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{X}\\in \\mathbb{R}^{N \\times M} $ and $\\mathbf{Y} \\in \\mathbb{R}^{N \\times 1}$ are matrix representations of training input variables  $\\{{\\mathbf{x}}\\}_{i = 1}^{N}$ and prediction variables $\\{{y}\\}_{i = 1}^{N}$ respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzmKef0IbBW2"
   },
   "source": [
    "\n",
    "## Finding the best $\\hat{\\theta}$\n",
    "\n",
    "Now we need to find the best parameter $ \\hat{\\theta} $ that minimizes the loss $J(\\theta)$. Here we introduce **two** ways to minimize the fuction: 1) by solving the Loss function's first-order derivative equation; 2) by Gradient Descent: a first-order iterative optimization algorithm. \n",
    "\n",
    "\n",
    "<table><tr>\n",
    "\n",
    "<td> <img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" alt=\"Drawing\" style= width=\"200\" height=\"200\"/> </td>\n",
    "\n",
    "<td> <img src=\"https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png\" alt=\"Drawing\" stylewidth=\"200\" height=\"200\"/> </td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "### 1) Solving the Loss function's first-order derivative equation\n",
    "\n",
    "One advantage of LR is that its Loss function's first-order derivative equation is solvable. Using the deferentiation rules we leared in high school, we can easily get the function's derivative :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_i \\mathbf{x}^{(i)}_k \\left(f_\\hat{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)}\\right) = 0\n",
    "\\end{equation}\n",
    "\n",
    "making $\\hat{\\theta}$ as a subject, we can rearrange the equation to :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta}_{k} = \\frac{\\sum_{i = 0}^{N}\\mathbf{x}_{k}^{(i)}y^{(i)} }{\\sum_{i = 0}^{N}(\\mathbf{x}_k^{(i)})^2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### 2) Gradient Descent\n",
    "\n",
    "Another way to find the optimum $\\hat{\\theta}$ is by using Gradien Descent. Gradient Descent finds the minimum of a Loss function by repeatly taking steps in the opposite direction of the gradient of the function at the current point, because this is the direction of steepest descent. Overtime, the weights of the model navigates to a point where the loss is very close to zero. We call this point a local minima of a loss function. \n",
    "\n",
    "Formally, Given a parameter set $\\theta_{t}$ at time $t$, we compute the next parameter set $\\theta_{t+1}$ as :\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} \\leftarrow \\theta_{t} + \\alpha *\\triangledown_{\\theta} J(\\theta_{t}) \n",
    "\\\\\n",
    "\\theta_{0} \\sim N(0,\\frac{I^{M \\times M}}{M})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is a learning rate usually set within $[0.00001, 0.1]$ and $\\triangledown_{\\theta} J(\\theta_{t}) $ is a Jacobian of the loss at time $t$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\triangledown_{\\theta} J(\\theta_{t}) = [\\frac{\\partial J(\\theta)}{\\partial \\theta_0}, \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, ..., \\frac{\\partial J(\\theta)}{\\partial \\theta_M}]^T \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## To matrix form\n",
    "\n",
    "Again, it is a good practice to translate the equations above to a matrix form :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\triangledown_{\\theta} J(\\theta_{t}) = \\mathbf{X}^T( f_{\\theta_t}(\\mathbf{X}) - \\mathbf{Y})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmHvNO6Mpe8C"
   },
   "source": [
    "## Traslating Maths to Code\n",
    "\n",
    "Now we are ready to translate the mathematical functions to programming functions. For this tutorial, we will use Python with Pytorch API. [Pytorch](https://pytorch.org/) is a Deep learning framework developed by facebook to accelerate Deep learning research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1zn0hUwK4UZ"
   },
   "source": [
    "### basic math operations in Pytorch\n",
    "\n",
    "Pytorch provides many useful basic matrix operations such as multiplication, transpose, inversion, transpose etc. **To use these operations we first need to  declare our data as a tensor datatype**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1IZlHgGM8QG"
   },
   "outputs": [],
   "source": [
    "# call a package\n",
    "import torch \n",
    "\n",
    "# generate a tensor\n",
    "X1 = torch.randn(3,3)\n",
    "# transpose\n",
    "O1 = X1.t()\n",
    "# multiplication\n",
    "O2 = X1 @ torch.transpose(X1, 1, 0) \n",
    "# inverse\n",
    "O3 = X1.inverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iblDFoPQ5ax"
   },
   "source": [
    "Now, lets convert maths to codes\n",
    "\n",
    "1. Inference :\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{X}) = f( \\mathbf{X} ;\\theta )  = \\mathbf{X}\\mathbf{\\theta}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1kGSaBHTjTA"
   },
   "outputs": [],
   "source": [
    "def f(X, Theta):\n",
    "  return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I15OBJ4oVG6l"
   },
   "source": [
    "2.Loss:\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{2N} (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})^T(\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyPQ37vdVQ8H"
   },
   "outputs": [],
   "source": [
    "def J(Y, YHat):\n",
    "  return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSoo_NxwXkbg"
   },
   "source": [
    "3. Jacobian\n",
    "\n",
    "\\begin{equation}\n",
    "\\triangledown_{\\theta} J(\\theta_{t}) = \\mathbf{X}^T(f_{\\theta_t} - \\mathbf{Y}(\\mathbf{X}))\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_acW8PyX5y9"
   },
   "outputs": [],
   "source": [
    "def jacob(X, Y, YHat):\n",
    "  return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh8xA6Wg05G9"
   },
   "source": [
    "4. Gradient Descent\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} \\leftarrow \\theta_{t} + \\alpha *\\triangledown_{\\theta} J(\\theta_{t}) \n",
    "\\\\\n",
    "\\theta_{0} \\sim N(0,\\frac{I^{M \\times M}}{M})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG3u6jyC00rj"
   },
   "outputs": [],
   "source": [
    "def GD(X, Y, lr = 0.00001, nIter = 1000):\n",
    "\n",
    "  N, M = X.shape\n",
    "\n",
    "  for n in range(nIter):\n",
    "    \n",
    "    if n == 0 : ThetaNow = torch.randn(M,1)/M\n",
    "    else      : ThetaNow = ThetaNext    \n",
    "    \n",
    "    ThetaNext = ThetaNow - lr*jacob(X, Y, f(X, ThetaNow))\n",
    "\n",
    "  Theta = ThetaNext\n",
    "\n",
    "  return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZiZ4kCXfPCR"
   },
   "source": [
    "4.1 Batch Gradient descent\n",
    "\n",
    "In practice, the size of data $ \\{(\\mathbf{x}^{(i)}, y^{(i)} )\\}_{i = 1}^{N} $ could so large that it can't fit into a computer's memory. An alternative is to randomly sample a subset (batch) of the dataset for every interation. We call this technique as Batch Gradient descent. Bigger batch size is prefered for a faster and more stable training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G8VqLgsjwD6"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def BGD(X, Y,\n",
    "        lr = 0.00001,\n",
    "        nIter = 1000,\n",
    "        batchProportion = 0.1 # 10 percent\n",
    "        ):\n",
    "\n",
    "  N, M = X.shape\n",
    "\n",
    "  batchSize = int(N * batchProportion)\n",
    "\n",
    "  for n in range(nIter):\n",
    "\n",
    "    if n == 0 : ThetaNow = torch.randn(M,1)/M\n",
    "    else      : ThetaNow = ThetaNext\n",
    "\n",
    "    idx = np.random.choice(range(N), size=batchSize)\n",
    "\n",
    "    X_batch = X[idx]\n",
    "    Y_batch = Y[idx]\n",
    "    \n",
    "    ThetaNext = ThetaNow - lr*jacob(X_batch, Y_batch, f(X_batch, ThetaNow))\n",
    "\n",
    "  Theta = ThetaNext\n",
    "\n",
    "  return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLG0LNNKvX92"
   },
   "source": [
    "4.1 Batch Gradient descent with momentum\n",
    "\n",
    "Graident Descent algorithm is memeryless: its weight update rule solely depends on the gradient $ \\triangledown_{\\theta} J(\\theta_{t}) $ that is computed at time $t$. As a result GD tends to oscillate across non-informative slopes while only making small progress towards reaching the good local minima. \n",
    "\n",
    "Momentum is a method that helps accelerate GD in the relevant direction and dampens oscillations. It does this by caching previous updates with a constant $γ$ :\n",
    "\n",
    "\\begin{equation}\n",
    "v_{t+1} \\leftarrow γv_{t} + \\alpha *\\triangledown_{\\theta} J(\\theta_{t}) \\;\\;\\;\\;\\;\\;\\ (Step \\; 1)\n",
    "\\\\\n",
    "\\theta_{t+1} \\leftarrow \\theta_{t} -v_{t+1} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ (Step \\; 2)\n",
    "\\\\\n",
    "\\theta_{0} \\sim N(0,\\frac{I^{M \\times M}}{M})\n",
    "\\\\\n",
    "v_{0} = 0^{1 \\times M}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2TZrk-k3VVU"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def MBGD(X, Y,\n",
    "         lr = 0.00001,\n",
    "         r  = 0.9,\n",
    "         nIter = 1000,\n",
    "         batchProportion = 0.1 # 10 percent\n",
    "        ):\n",
    "\n",
    "  N, M = X.shape\n",
    "\n",
    "  batchSize = int(N * batchProportion)\n",
    "\n",
    "  for n in range(nIter):\n",
    "\n",
    "    if n == 0 : \n",
    "      ThetaNow = torch.randn(M,1)/M\n",
    "      VNow     = torch.zeros(M,1)\n",
    "    else      : \n",
    "      ThetaNow = ThetaNext\n",
    "      VNow     = VNext\n",
    "\n",
    "    idx = np.random.choice(range(N), size=batchSize)\n",
    "    \n",
    "    X_batch = X[idx]\n",
    "    Y_batch = Y[idx]\n",
    "    \n",
    "\n",
    "    VNext = r*VNow + lr*jacob(X_batch, Y_batch, f(X_batch, ThetaNow))    \n",
    "    ThetaNext = ThetaNow - VNext\n",
    "    \n",
    "  Theta = ThetaNext\n",
    "\n",
    "  return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aa-m94VeLDt"
   },
   "source": [
    "5. closed form solver\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP8pVTtWeGU2"
   },
   "outputs": [],
   "source": [
    "def CF(X, Y):\n",
    "  return (X.t() @ X).inverse() @ X.t() @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb6kCpy_Z3ZZ"
   },
   "source": [
    "# Download and process data\n",
    "This [dataset](https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt) is a modified version of the California Housing dataset available from Luís Torgo's page (University of Porto). Luís Torgo obtained it from the StatLib repository (which is closed now). The dataset may also be downloaded from StatLib mirrors.\n",
    "\n",
    "This dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"100%\" src=\"https://www.geocurrents.info/wp-content/uploads/2016/01/California-Average-Home-Price-Map.png\"> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha_QCOOyQJq5"
   },
   "source": [
    "### downloading and processing datasets\n",
    "\n",
    "Since the dataset is relatively small (20443 rows and 11 colums), we will directly download the dataset from it's url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "VSqKbRk8rwJB",
    "outputId": "182c99c2-d76e-47d6-ad31-56e8448930b0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "mTRBW3hKtEIF",
    "outputId": "81cf6d6b-654c-48ec-d648-714682ed5bf1"
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn74CPZxuJ_q"
   },
   "source": [
    "Now lets do some feature selection and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEfQHEeItgZR"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(how='any')\n",
    "\n",
    "X = df[ [\"longitude\", \n",
    "          \"latitude\",\n",
    "          \"total_rooms\",\n",
    "          \"total_bedrooms\",\n",
    "          \"population\", \n",
    "          \"households\"] ]\n",
    "\n",
    "conX = df[ [\"longitude\",\n",
    "            \"latitude\",\n",
    "            \"total_rooms\",\n",
    "            \"total_bedrooms\",\n",
    "            \"population\", \n",
    "            \"households\"]].apply(lambda x: (x-x.mean()) / x.std(), axis=0)\n",
    "\n",
    "catX = pd.get_dummies( df[\"ocean_proximity\"], prefix='ocean_proximity', drop_first=True)\n",
    " \n",
    "\n",
    "X = pd.concat([conX, catX] , axis = 1)\n",
    "\n",
    "Y = df[[\"median_house_value\"]].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "X = torch.tensor(X.to_numpy()).type(torch.float32)\n",
    "Y = torch.tensor(Y.to_numpy()).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9yudKkzuYpu"
   },
   "source": [
    "We are all set! We have the $\\mathbf{X}$ and $\\mathbf{Y}$ which are the only inputs required for training our LR model. Lets start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPSfSIBfAsyz",
    "outputId": "87832415-ee8c-48d4-8b4c-37c51f620079"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "XTrain, XTest = X[: int(len(X) * 0.8) ], X[int(len(X) * 0.8) + 1 :]\n",
    "YTrain, YTest = Y[: int(len(X) * 0.8) ], Y[int(len(X) * 0.8) + 1 :]\n",
    "\n",
    "# worst approximation\n",
    "ThetaRandom = torch.randn(XTest.shape[1], 1)\n",
    "\n",
    "print(\"training with GD ...\")\n",
    "start = time()\n",
    "ThetaGD = GD(XTrain, YTrain , lr=1e-05, nIter=1000)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "\n",
    "print(\"training with BGD ...\")\n",
    "start = time()\n",
    "ThetaBGD = BGD(XTrain, YTrain, lr=1e-05, nIter=1000, batchProportion = 0.01)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "print(\"training with MBGD ...\")\n",
    "start = time()\n",
    "ThetaBGD = MBGD(XTrain, YTrain, lr=1e-05, nIter=1000, batchProportion = 0.01)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "print(\"training with CF ...\")\n",
    "start = time()\n",
    "ThetaCF = CF(XTrain, YTrain)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c0SGaGQpIo6",
    "outputId": "67867d62-033a-4759-f073-2c0a0e4810a5"
   },
   "outputs": [],
   "source": [
    "YHatTestRandom = f(XTest, ThetaRandom)\n",
    "YHatTestGD     = f(XTest, ThetaGD)\n",
    "YHatTestBGD    = f(XTest, ThetaBGD)\n",
    "YHatTestMBGD   = f(XTest, ThetaBGD)\n",
    "YHatTestCF     = f(XTest, ThetaCF)\n",
    "\n",
    "print( f\"testLoss with random : {J(YTest,YHatTestRandom)}\" )\n",
    "print( f\"testLoss with GD     : {J(YTest,YHatTestGD)}\" )\n",
    "print( f\"testLoss with BGD    : {J(YTest,YHatTestBGD)}\" )\n",
    "print( f\"testLoss with MBGD   : {J(YTest,YHatTestMBGD)}\" )\n",
    "print( f\"testLoss with CF     : {J(YTest,YHatTestCF)}\" )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "aAUE4hqdgV5W",
    "yzmKef0IbBW2"
   ],
   "name": "Gradient_Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "miccai",
   "language": "python",
   "name": "miccai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
