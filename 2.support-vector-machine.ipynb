{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "For this hands on, we will learn SVM unsing Pytorch. I will now assume that the readers are already familar with 1) Pytorch, 2) Regression models and 3) MNIST data.\n",
    "\n",
    "OBJECTIVE:\n",
    "    * SVM using a standard pytorch ML framework\n",
    "    * Aplying the model to predict binary digit (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: definition\n",
    "\n",
    "Just as in the case of Logistic Regression, SVM also aims to model a fuction $f_{w,b}$ that predicts a binary label $ y\\in[-1,+1]$ given a continuous data $x \\in \\mathbb{R}^{K}$ where K is the number of features in $x$. However unlike the Regression mdoels we learnt, SVM tries to answer the problem from a geometic perspective. \n",
    "\n",
    "As shown in the figure below, lets imagine we have n data points that are scattered around in a 2D space and we have labels denoted with different colors: blue for positives (+1) and red for negatives (-1). One way to build a good classifer is to find the line (or hyperplane) : $ wx + b = 0 $ that has the largest distance to the nearest postive and negative samples (a.k.a support vectors). where $w$ is the normal vector and $b$ is a scalar offset.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"50%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/600px-SVM_margin.png\"> \n",
    "</p>\n",
    "\n",
    "So how do we translate the geometric concept to a set of equations? Let's first define a decison rule for SVM. Given a hyperplane $ wx + b = 0 $, Our decision rule is as follows, for each $i$ :\n",
    "\n",
    "\\begin{equation}\n",
    "wx_{i}+b \\geq 1, \\;\\;\\; \\text{if} \\;\\;\\; y_{i} = +1\n",
    "\\\\\n",
    "wx_{i}+b \\leq 1, \\;\\;\\; \\text{if} \\;\\;\\; y_{i} = -1\n",
    "\\end{equation}\n",
    "\n",
    "Where the constant 1 is a value to prevent datapoints from falling into the margin. The above equation can be written more compactly as:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i(wx_{i}+b) \\geq 1, \\forall i\n",
    "\\end{equation}\n",
    "\n",
    "As mentioned earier, our goal is to find the hyperplane $ wx + b = 0 $ that has the largest margin to support vectors: $ (x_{+ve} - x_{-ve}) \\cdot \\frac{w}{||w||}$. Where $x_{+ve}$ and $x_{-ve}$ are the positive and negative datapoints that lie on the either side of support vectors lines. Using the the decision rule above, we can re-write the margin simply as $\\frac{2}{||W||}$. We want to maximize the margin $\\frac{2}{||W||}$ or simply minimize its inverse $ ||w||$. Finally putting a constraint above that the decision rule has to be greater or equal to 1: $1 - (wx_{i}+b) \\leq 0 $, The final objective function is:\n",
    "\n",
    "\\begin{equation}\n",
    "J(w,b) = ||w|| + C\\sum_{i} max[0, 1 - y_{i} ( wx_{i} - b ) ]\n",
    "\\end{equation}\n",
    "\n",
    "where $C$ is an hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first import packages we will use\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create BatchLoader\n",
    "Just as the Logistic Regression exercise, we will use MNIST dataset. Lets preprocess the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download \n",
    "trainD = MNIST(\".\", download = True, train = True)\n",
    "testD  = MNIST(\".\", download = True, train = False)\n",
    "\n",
    "# to tensor\n",
    "toTensor = lambda pair: (transforms.ToTensor()(pair[0]), pair[1])\n",
    "trainD = map(toTensor, trainD)\n",
    "testD  = map(toTensor, testD)\n",
    "\n",
    "# subsetting 0 and 1\n",
    "only01 = lambda pair : pair[1] in [0,1]\n",
    "trainD = filter(only01, trainD)\n",
    "testD  = filter(only01, testD)\n",
    "\n",
    "# 0 to -1 \n",
    "otom1 = lambda pair : (pair[0], -1 if pair[1] == 0 else 1)\n",
    "trainD = map(otom1, trainD)\n",
    "testD  = map(otom1, testD)\n",
    "\n",
    "\n",
    "# normalisation\n",
    "normalize = lambda pair: (transforms.Normalize(mean=[0], std=[1])(pair[0]), pair[1] )\n",
    "trainD = map(normalize, trainD)\n",
    "testD  = map(normalize, testD)\n",
    "\n",
    "# flatten\n",
    "flatten = lambda pair: ( torch.flatten(pair[0]), pair[1] )\n",
    "trainD = map(flatten, trainD)\n",
    "testD  = map(flatten, testD)\n",
    "\n",
    "processedTrainD = list(trainD)\n",
    "processedTestD  = list(testD)\n",
    "\n",
    "# batch loader\n",
    "trainX = torch.stack(list(map(lambda pair : pair[0], processedTrainD)))\n",
    "trainY = torch.tensor(list(map(lambda pair : pair[1], processedTrainD)))\n",
    "\n",
    "trainLoader = DataLoader( TensorDataset(trainX, trainY), batch_size = 100)\n",
    "\n",
    "testX = torch.stack(list(map(lambda pair : pair[0], processedTestD)))\n",
    "testY = torch.tensor(list(map(lambda pair : pair[1], processedTestD)))\n",
    "\n",
    "testLoader = DataLoader( TensorDataset(testX, testY), batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and train a model\n",
    "\n",
    "You will notice that the code implementation of SVM and Logistic Regression and nearly identical with only 2 differences: 1) SVM does not have a non-linear function after a linear mapping and 2) We use the loss introduced above (also known as Hinge Loss) instead of Binary Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch:0 = 7.115142087767444\n",
      "Loss at epoch:1 = 1.264043013880572\n",
      "Loss at epoch:2 = 0.984408650341935\n",
      "Loss at epoch:3 = 0.8430878304121062\n",
      "Loss at epoch:4 = 0.7480847816767655\n",
      "Loss at epoch:5 = 0.6820956585914131\n",
      "Loss at epoch:6 = 0.634114039695169\n",
      "Loss at epoch:7 = 0.599269598018466\n",
      "Loss at epoch:8 = 0.5742188199298588\n",
      "Loss at epoch:9 = 0.5517017996217323\n",
      "Loss at epoch:10 = 0.533276734858986\n",
      "Loss at epoch:11 = 0.5177706278215243\n",
      "Loss at epoch:12 = 0.5033151887533233\n",
      "Loss at epoch:13 = 0.4909841854741254\n",
      "Loss at epoch:14 = 0.47998717262988955\n",
      "Loss at epoch:15 = 0.4694254304480365\n",
      "Loss at epoch:16 = 0.460208200563596\n",
      "Loss at epoch:17 = 0.45090110893324603\n",
      "Loss at epoch:18 = 0.44240734098464485\n",
      "Loss at epoch:19 = 0.4340640620922479\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(28*28, 1)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr = 0.0001, momentum = 0.0)\n",
    "\n",
    "for e in range(20):\n",
    "    losses = []\n",
    "    for X,y in trainLoader:    \n",
    "\n",
    "        X = X.type(torch.float32)\n",
    "        y = y.type(torch.float32)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yHat = model(X)\n",
    "\n",
    "        loss = torch.sum(torch.clamp(1 - yHat.t()*y, min = 0))\n",
    "      \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()                        \n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Loss at epoch:{e} = {np.mean(losses)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "Again it is almost identical to Logistic Regression excpet that we now use a sign function instead of rounding to 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc : 99.9 %\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "yHats = []\n",
    "ys    = []\n",
    "for X,y in testLoader:\n",
    "\n",
    "    X = X.type(torch.float32)\n",
    "    y = y.type(torch.float32)\n",
    "    \n",
    "    model.eval()\n",
    "    yHat = model(X)\n",
    "    \n",
    "    ys.append(y.detach().numpy())\n",
    "    yHats.append(yHat.detach().numpy())\n",
    "    \n",
    "yHats = np.concatenate(yHats)\n",
    "ys    = np.concatenate(ys)\n",
    "\n",
    "acc = sum((np.sign(yHats)).squeeze() == ys) / len(ys) * 100 \n",
    "\n",
    "print(f\"test acc : {str(acc)[:4]} %\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-1.13",
   "language": "python",
   "name": "tf-1.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
