{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language Translation with Transformer\n",
        "\n",
        "In this hands-on session, we demonstrate how to build a light transformer model to translate German to English. Codes below are adopted from this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first install the 'spacy' package, which provides handy tools to break sentences into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /home/hongfei/anaconda3/lib/python3.8/site-packages (3.2.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (2.0.1)\n",
            "2022-03-09 21:10:12.322015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-03-09 21:10:12.322050: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 469 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: setuptools in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (52.0.0.post20210125)\n",
            "Requirement already satisfied: jinja2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.6)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.5.30)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2022-03-09 21:10:31.689681: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-03-09 21:10:31.689720: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Collecting de-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 412 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from de-core-news-sm==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: setuptools in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (52.0.0.post20210125)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.20.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.62.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: jinja2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.10.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.6)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.0.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/hongfei/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Language Translation with nn.Transformer and torchtext\n",
        "\n",
        "We will be using [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1)\n",
        "dataset to train a German to English translation model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Sourcing and Processing\n",
        "\n",
        "The following block of codes perform the following\n",
        "1.  Load the German--English pairs of sentences from Multi30k\n",
        "2.  Use tokenizer to break sentences into words\n",
        "3.  Build German and English vocabularies from corresponding words\n",
        "4.  Define special 'words'\n",
        "    -  BOS: Begin of Sentence\n",
        "    -  EOS: End of Sentence\n",
        "    -  PAD: Padding\n",
        "    -  UNK: Unkown word (for words not in training vocabularies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        " \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator \n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object \n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seq2Seq Network using Transformer\n",
        "\n",
        "PyTorch has its build-in transformer constructor. We need to implement our own word embedding and position embedding methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network \n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some technical details. Suppose the English target sentence is 'I am a student.'\n",
        "-  'I am a student' is target output\n",
        "-  However, we want to predict the sentence from left to right\n",
        "    - (Ich bin ein Student) -> I\n",
        "    - (Ich bin ein Student) + I -> (I am)\n",
        "    - (Ich bin ein Student) + (I am) -> (I am a)\n",
        "    - ...\n",
        "-  When trained to predict 'a', our model SHOULD NOT know the right part of the sentence 'student'\n",
        "-  The following codes build masks to mask right part of sentence when training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now define the parameters of our model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 4\n",
        "FFN_HID_DIM = 256\n",
        "BATCH_SIZE = 30\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Collation\n",
        "\n",
        "Functions to prepare data to feed into our transformer for training.\n",
        "\n",
        "-  PyTorch can train on a batch of sentences to speed up training\n",
        "-  For one batch\n",
        "    -   We pad shorter sentences so that all sentences have the same length\n",
        "    -   We insert special words (BOS, EOS, PAD) into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define training and evaluation loop that will be called for each \n",
        "epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    \n",
        "    for src, tgt in tqdm(train_dataloader):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        \n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the actual function to translate German sentences to English sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm \n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us initialize an instance of our transformer model and use it to translate a German sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated sentence for randomly initialized transformer\n",
            "------------------------------\n",
            " Anaheim travel blocked blocked blocked blocked blocked blocked blocked blocked blocked blocked blocked blocked blocked\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "print('Translated sentence for randomly initialized transformer')\n",
        "print('-'*80)\n",
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the cross entropy as the loss function, and use Adam (a variant of SGD with momentum) as the optimizer. Now we have all the ingredients to train our model. Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc46ab5c8a604922a24e3381b2c5f7ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/967 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 5.675, Val loss: 4.410, Epoch time = 211.850s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0c730dd44564e4c813b48ef581ccc1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/967 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2, Train loss: 4.163, Val loss: 3.844, Epoch time = 212.395s\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d528afa1c744cb0b98b323f64d91c27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/967 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3, Train loss: 3.740, Val loss: 3.493, Epoch time = 214.444s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated sentence for trained transformer\n",
            "--------------------------------------------------------------------------------\n",
            " A group of people are standing in a group of people . \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print('Translated sentence for trained transformer')\n",
        "print('-'*80)\n",
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "print('-'*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
