{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Topics:\n",
    "1.  Linear SVM\n",
    "    -  Model fitting\n",
    "    -  Separating plane visualization\n",
    "2.  SVM with kernel\n",
    "    -  Model fitting\n",
    "    -  Decision function visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "The linear SVM uses a plane to separate two classes of data points. This model works well when the two classes are linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us load and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "# If you are using Colab,\n",
    "# don't forget to upload the .csv files to\n",
    "# sample_data directory.\n",
    "data = pd.read_csv('sample_data/linearly_separable.csv',header=0, names=['x0','x1','label'])\n",
    "data['label'] = data['label'].astype('str')\n",
    "\n",
    "x_all = data[['x0','x1']].to_numpy()\n",
    "label_all = data[['label']].to_numpy().flatten()\n",
    "\n",
    "# Split data and dataframe\n",
    "id = np.arange(len(label_all))\n",
    "x, x_t, label, label_t, id, id_t = train_test_split(x_all, label_all, id, random_state=5)\n",
    "data_train, data_test = data.iloc[id], data.iloc[id_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization\n",
    "fig = px.scatter(data_train, x='x0', y='x1', color='label', title='Training Data')\n",
    "fig.update_yaxes(\n",
    "    scaleanchor='x',\n",
    "    scaleratio=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sklearn library to build our linear SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "linear_SVM = svm.SVC(kernel='linear')\n",
    "linear_SVM.fit(x,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fitted model on the testing dataset\n",
    "pred = linear_SVM.predict(x_t)\n",
    "acc = (pred==label_t).sum()/len(label_t)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the fitted linear SVM has an accuracy of $100\\%$ on the testing dataset. This is because the two classes in this dataset are linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily get the separating line from our fitted model. Recall that the line has the form\n",
    "$$\n",
    "w_0 x_0 + w_1 x_1 + b = 0,\n",
    "$$\n",
    "where $w=(w_0,w_1)$ is the coefficient, and $b$ is the intercept of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **decision function** of our linear SVM is given by\n",
    "$$\n",
    "f((x_0,x_1)) = w_0 x_0 + w_1 x_1 + b.\n",
    "$$\n",
    "A data $(x_0,x_1)$ belongs to the positive class if $f((x_0,x_1))>0$, and it belongs to the negative class if $f((x_0,x_1))<0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = linear_SVM.coef_.flatten()\n",
    "b = linear_SVM.intercept_.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the separating line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_min, x0_max = x_all[:,0].min(), x_all[:,0].max()\n",
    "x1_min, x1_max = -1/w[0]*(w[1]*x0_min+b), -1/w[0]*(w[1]*x0_max+b)\n",
    "\n",
    "fig = px.scatter(data_train, x='x0', y='x1', color='label')\n",
    "fig.update_yaxes(\n",
    "    scaleanchor='x',\n",
    "    scaleratio=1\n",
    ")\n",
    "fig.add_traces(go.Scatter(x=[x0_min, x0_max],y=[x1_min, x1_max], mode='lines', name='separating line'))\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Kernel\n",
    "\n",
    "It is rare to have linearly separable classes in everyday line.\n",
    "\n",
    "A linear SVM only uses **one** separating line to separate the two classes, it will fail for data that is highly non-linear.\n",
    "\n",
    "We will demonstrate this by using a simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('sample_data/circular.csv',header=0, names=['x0','x1','label'])\n",
    "data['label'] = data['label'].astype('str')\n",
    "\n",
    "x_all = data[['x0','x1']].to_numpy()\n",
    "label_all = data[['label']].to_numpy().flatten()\n",
    "\n",
    "# Split data and dataframe\n",
    "id = np.arange(len(label_all))\n",
    "x, x_t, label, label_t, id, id_t = train_test_split(x_all, label_all, id, random_state=5)\n",
    "data_train, data_test = data.iloc[id], data.iloc[id_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_train,\n",
    "    x='x0',\n",
    "    y='x1',\n",
    "    color='label',\n",
    "    title='Training Data'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    scaleanchor='x',\n",
    "    scaleratio=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the two classes in this dataset is NOT linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of the Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we fit a linear SVM to this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_SVM = svm.SVC(kernel='linear')\n",
    "linear_SVM.fit(x,label)\n",
    "\n",
    "# Test fitted model on the testing dataset\n",
    "pred = linear_SVM.predict(x_t)\n",
    "acc = (pred==label_t).sum()/len(label_t)\n",
    "print('Accuracy of linear SVM on circular test datapoints is : {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = linear_SVM.coef_.flatten()\n",
    "b = linear_SVM.intercept_.item()\n",
    "\n",
    "x0_min, x0_max = x_all[:,0].min(), x_all[:,0].max()\n",
    "x1_min, x1_max = -1/w[0]*(w[1]*x0_min+b), -1/w[0]*(w[1]*x0_max+b)\n",
    "\n",
    "fig = px.scatter(data_train, x='x0', y='x1', color='label')\n",
    "fig.update_yaxes(\n",
    "    scaleanchor='x',\n",
    "    scaleratio=1\n",
    ")\n",
    "fig.add_traces(go.Scatter(x=[x0_min, x0_max],y=[x1_min, x1_max], mode='lines', name='separating line'))\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Non-linear Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a non-linear kernel to separate the two classes in the circular dataset. Recall that in the lecture, you learned to use Gaussian similarity kernel\n",
    "$$\n",
    "K(x^{(i)},x^{(j)}) = \\exp\\left(-\\gamma||x^{(i)}-x^{(j)}||^2\\right).\n",
    "$$\n",
    "This kernel is also widely known as the **radial basis function** (rbf) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_SVM = svm.SVC(kernel='rbf')\n",
    "rbf_SVM.fit(x,label)\n",
    "\n",
    "# Test fitted model on the testing dataset\n",
    "pred = rbf_SVM.predict(x_t)\n",
    "acc = (pred==label_t).sum()/len(label_t)\n",
    "print('Accuracy of rbf SVM on circular test datapoints is : {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the rbf kernel, our SVM model can make perfect prediction on the test dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rbf SVM no longer uses lines to separate classes, and the decision function $f$ is very complicated.\n",
    "\n",
    "Our fitted rbf_SVM model can help us calculate the decision function. The classification rule is the same as before:\n",
    "1.  $(x_0,x_1)$ is the negative class if $f((x_0,x_1))<0$.\n",
    "1.  $(x_0,x_1)$ is the positive class if $f((x_0,x_1))<0$.\n",
    "\n",
    "We visualize the decision function in the following way.\n",
    "1.  We uniformly sample points on the plane around our data points.\n",
    "2.  We evaluate the decision function on each sampled points.\n",
    "3.  We make a 3D plot to visualize the decision function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0_min, x_0_max = data['x0'].min(), data['x0'].max()\n",
    "x_1_min, x_1_max = data['x1'].min(), data['x1'].max()\n",
    "\n",
    "# We sample 100 data points on each axis.\n",
    "# Then we use the meshgrid method to generate\n",
    "# coordinates of the points.\n",
    "x0_sample, x1_sample = np.meshgrid(\n",
    "    np.linspace(x_0_min, x_0_max, 100),\n",
    "    np.linspace(x_1_min, x_1_max, 100)\n",
    ")\n",
    "\n",
    "x0_sample_flat, x1_sample_flat = x0_sample.flatten(), x1_sample.flatten()\n",
    "plane_samples = np.vstack([x0_sample_flat, x1_sample_flat]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_samples[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_decision_values = rbf_SVM.decision_function(plane_samples).reshape(x0_sample.shape)\n",
    "data_decision_values = rbf_SVM.decision_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Scatter3d(x=x[:,0],y=x[:,1],z=data_decision_values, mode='markers', marker=dict(size=5,color=label.astype(np.int32))))\n",
    "fig.add_traces(\n",
    "    go.Surface(x=x0_sample,y=x1_sample,z=sampled_decision_values,opacity=0.5,showscale=False)\n",
    ")\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.update_layout(scene_aspectmode='manual',\n",
    "                  scene_aspectratio=dict(x=1, y=1, z=1),\n",
    "                  title='Decision Function Visualization')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the decision function (represented by the surface) is non-linear. Points of the $+1$ class have positive decision values, while points of the $-1$ class have negative decision values."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e84f0b0af5ca574cc936389bd8ce930b03b6ba66c346a207ba44665da503187e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
