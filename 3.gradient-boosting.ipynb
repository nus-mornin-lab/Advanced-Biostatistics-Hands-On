{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting (Adaboost)\n",
    "\n",
    "For this hands on, we will define Gradient Boosting. I will asumme that the readers are already familiar with 1) Python; 2) decision tree model and 3) MNIST dataset.\n",
    "\n",
    "OBJECTIVE:\n",
    "    * Gradient Boosting (Adaboost) using Scikit-learn.\n",
    "    * Aplying the model to predict binary digit (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (Adaboost)\n",
    "\n",
    "Adaboost is much stronger than the Logistic Regresssion or the SVM we learnt. Adaboost is highly non-linear and yet does not suffer much from overfitting. Therefore Adaboost is considered as an out-of-the-box machine learning that you can expect to get a decent performance on various challenging tasks.\n",
    "\n",
    "The motivation behind Adaboost algorithm is to \"Iteratively boost the performance of a model by penalizing more on misclassified observations\". The keyword here is **Iteratively**. Both Logistic Regression and SVM are one-shot classifiers: they have one model to predict the target variable $y$ and we don't do anyhing further on the misclassification errors made $e = y - \\hat{y}$.\n",
    "\n",
    "As shown in figure, Adaboost works with a set of weak classifiers $\\{G_{m}\\}_{m=1}^{M}$(for this tutorial, we define the classifiers with simple tree models). A classifier at time = $m$: $G_{m}$ tries to refine from the error made at the previous model iteration $y \\neq G_{m-1}(x)$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"50%\" src=\"https://lh4.googleusercontent.com/GLcjrWSUkjadvf0hTV_y7ATgh7l-UQQ12_UYluxQYxxWvSKoP5AJN6cvKS5s-uvO_kR3OVBlgL6Q0MATYoueKF59-eIO718Fz9KsVVcObbO54OhfIEkEYlWrn6vA2rr4qXfn2rbsIkMMnHuWEQ\n",
    "\"> \n",
    "</p>\n",
    "\n",
    "### Adaboost Algorithm\n",
    "\n",
    "* Initialize the observation weights $w_i = 1/n, i = 1, ..., n$. \n",
    "* For m = 1 to M (pre-chosen number of iterations):\n",
    "    - Fit a classifier G_m(x) to the training data using weights w_{i}\n",
    "    - Compute $ err_{m} = \\frac{ \\sum_{i} w_{i} I \\{ y_i \\neq G_{m}(x_i) \\} } {\\sum_i w_i} $\n",
    "    - Compute $a_m = \\text{log}_e(\\frac{1 - err_m}{err_m} ) $\n",
    "    - Update weights: $ w_{i}^{new} \\leftarrow w_{i} \\cdot \\text{exp} [ \\alpha_{m} \\cdot I \\{ y_i \\neq G_m( x_i ) \\} ] $\n",
    "\n",
    "* Final prediction : $G(x) = \\text{sign} [ \\sum_i \\alpha_m G_m (x) ] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Code\n",
    "\n",
    "We are not using Pytorch this time because, Pytorch is developped majorly for designing deep learning models, Adaboost (especially with tree model) has very differnt flavor to the current deep learning standard which assumes gradient based optimizatation. Therefore we will use Scikit-learn packages that contains a collection of useful machine learning tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first import packages we will use\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download \n",
    "trainD = MNIST(\".\", download = True, train = True)\n",
    "testD  = MNIST(\".\", download = True, train = False)\n",
    "\n",
    "# to tensor\n",
    "toTensor = lambda pair: (transforms.ToTensor()(pair[0]), pair[1])\n",
    "trainD = map(toTensor, trainD)\n",
    "testD  = map(toTensor, testD)\n",
    "\n",
    "# subsetting 0 and 1\n",
    "only01 = lambda pair : pair[1] in [0,1]\n",
    "trainD = filter(only01, trainD)\n",
    "testD  = filter(only01, testD)\n",
    "\n",
    "# normalisation\n",
    "normalize = lambda pair: (transforms.Normalize(mean=[0], std=[1])(pair[0]), pair[1] )\n",
    "trainD = map(normalize, trainD)\n",
    "testD  = map(normalize, testD)\n",
    "\n",
    "# flatten\n",
    "flatten = lambda pair: (torch.flatten(pair[0]).detach().numpy(), pair[1] )\n",
    "trainD = map(flatten, trainD)\n",
    "testD  = map(flatten, testD)\n",
    "\n",
    "processedTrainD = np.array(list(trainD))\n",
    "processedTestD  = np.array(list(testD))\n",
    "\n",
    "XTrain, yTrain  = zip(*processedTrainD)\n",
    "XTest,  yTest   = zip(*processedTestD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=20)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the response for test dataset\n",
    "yPred = model.predict(XTest)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(yTest, yPred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-1.13",
   "language": "python",
   "name": "tf-1.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
