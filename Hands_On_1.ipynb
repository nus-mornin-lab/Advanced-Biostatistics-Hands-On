{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAUE4hqdgV5W"
   },
   "source": [
    "# Hands On Regression Analysis\n",
    "\n",
    "Scopes:\n",
    "  * Recap of linear regression\n",
    "  * Loss function of a LR model\n",
    "  * Learning with Gradient Descent\n",
    "  * Applying the above techniques to the [US housing data](https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv) using [Pytorch](https://pytorch.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly==5.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the following packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8m_AfeNYBf6"
   },
   "source": [
    "# Recap of linear regression\n",
    "\n",
    "Linear Regression (LR) is one of the simplest machine learning model for predicting continuous outcomes (e.g. time, length). As shown in the figure below, LR tries to learn the statistics (patterns) that exist in a dataset by trying to draw the best fitted line to the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"75%\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model formulation\n",
    "\n",
    "Linear regression is a supervised learning model. We are given a sequence of **dependent variable** $(\\mathbf x^{(i)})_{i=1}^N$ and the corresponding **labels** $(y^{(i)})_{i=1}^N$, where each dependent variable $\\mathbf x^{(i)}$ is a $M$-dimensional vector of the form\n",
    "\n",
    "$$\n",
    "\\mathbf x^{(i)} = (x_1^{(i)}, x_2^{(i)}, \\cdots, x_M^{(i)}),\n",
    "$$\n",
    "\n",
    "and each label $y^{(i)}$ is a real number. Then the linear regression model has the form (for convenience we define $x^{(i)}_0=1$)\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{x}^{(i)}) = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{M}x_{M} = \\sum_{k = 0}^{M}{ \\theta_{k} x_{k}^{(i)} },\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\theta_i$'s are parameters of the model to be determined by fitting the model to the given data. Our goal is to find the 'best' parameters $\\hat{\\theta}_i$ such that $f_{\\hat{\\theta}}(\\mathbf x^{(i)})\\sim y^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We find $\\hat{\\theta}$ by minimizing the **Mean Square Error** $ J(\\theta) $ between $f_{\\theta}(\\mathbf{x})$ and $y$:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\frac{1}{2N} \\sum_{i = 1}^{N} \\left( f_\\theta(\\mathbf{x}^{(i)}) - y^{(i)} \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "This function $J(\\theta)$ is called a **loss function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To matrix form\n",
    "\n",
    "It is often conventient to re-respresent math equations that deals with large amount of data as concise matrix operations. Therefore, the equations above can be re-written as (recall we have $\\mathbf{x}^{(i)}=(1,x^{(i)}_1,\\cdots, x^{(i)}_M)$):\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{x}^{(i)}) = \\mathbf{\\theta}^\\intercal \\mathbf{x}^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\frac{1}{2N} (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})^\\intercal (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{X}\\in \\mathbb{R}^{N \\times (M+1)} $ and $\\mathbf{Y} \\in \\mathbb{R}^{N \\times 1}$ are matrix representations of training input variables  $\\{{\\mathbf{x}^{(i)}}\\}_{i = 1}^{N}$ and prediction variables $\\{{y^{(i)}}\\}_{i = 1}^{N}$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzmKef0IbBW2"
   },
   "source": [
    "\n",
    "## Finding the best $\\hat{\\theta}$\n",
    "\n",
    "We minimize the loss function to obtain the best $\\hat{\\theta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = \\arg\\min_{\\theta} J(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Here we introduce two ways to minimize the fuction: 1) by Gradient Descent: a first-order iterative optimization algorithm; 2) by solving the Loss function's first-order derivative equation. \n",
    "\n",
    "\n",
    "<table><tr>\n",
    "\n",
    "<td> <img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" alt=\"Drawing\" style= width=\"200\" height=\"200\"/> </td>\n",
    "\n",
    "<td> <img src=\"https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png\" alt=\"Drawing\" stylewidth=\"200\" height=\"200\"/> </td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "### 1) Gradient Descent\n",
    "\n",
    "Gradient Descent finds the minimum of a loss function by repeatly taking steps in the opposite direction of the gradient of the function, because this is the direction of steepest descent. Overtime, the weights of the model navigates to a point where the loss is very close to its minimum.\n",
    "\n",
    "Formally, given a parameter $\\theta_{t}$ at time $t$, we compute the next parameter $\\theta_{t+1}$ as :\n",
    "\n",
    "\\begin{equation} \n",
    "\\theta_{t+1} \\leftarrow \\theta_{t} - \\eta * \\nabla_{\\theta} J(\\theta_{t})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta$ is a learning rate usually set within $[0.00001, 0.1]$ and $\\nabla_{\\theta} J(\\theta_{t}) $ is the Jacobian of the loss function\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J = [\\frac{\\partial J(\\theta)}{\\partial \\theta_0}, \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, ..., \\frac{\\partial J(\\theta)}{\\partial \\theta_M}]^\\intercal = \\frac{1}{N}\\mathbf X^\\intercal (\\mathbf X\\theta - \\mathbf Y)\n",
    "\\end{equation}\n",
    "\n",
    "evaluated at $\\theta_t$.\n",
    "\n",
    "### 2) Solving the Loss function's first-order derivative equation\n",
    "\n",
    "One advantage of linear regression is that its loss function's first-order derivative equation is solvable. Using the differentiation rules, we can easily get the function's first-order derivative equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_i \\mathbf{x}^{(i)}_k \\left(f_{\\hat{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)}\\right) = 0,\\quad k=0,1,\\cdots, M.\n",
    "\\end{equation}\n",
    "\n",
    "Solving these first-order equations gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^\\intercal \\mathbf{X})^{-1}\\mathbf{X}^\\intercal \\mathbf{Y}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmHvNO6Mpe8C"
   },
   "source": [
    "-----\n",
    "-----\n",
    "# Linear Regression in Python\n",
    "\n",
    "For this tutorial, we will use Python with Pytorch API to implement linear regression. [Pytorch](https://pytorch.org/) is a Deep learning framework developed by facebook to accelerate Deep learning research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1zn0hUwK4UZ"
   },
   "source": [
    "## Basic math operations in Pytorch\n",
    "\n",
    "Pytorch provides many useful basic matrix operations such as multiplication, transpose, inversion etc. **To use these operations we first need to  declare our data as a tensor datatype**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1IZlHgGM8QG"
   },
   "outputs": [],
   "source": [
    "# call a package\n",
    "import torch \n",
    "\n",
    "# generate a random tensor\n",
    "X1 = torch.randn(2,2)\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pointwise multiplication\n",
    "O0 = X1*X1\n",
    "print(O0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose\n",
    "O1 = X1.t()\n",
    "print(O1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "O2 = X1 @ torch.transpose(X1, 1, 0) \n",
    "print(O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse\n",
    "O3 = X1.inverse()\n",
    "print(O3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iblDFoPQ5ax"
   },
   "source": [
    "--------\n",
    "Now, lets convert maths to codes\n",
    "\n",
    "1. Inference :\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{X}) = f( \\mathbf{X} ;\\theta )  = \\mathbf{X}\\mathbf{\\theta}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1kGSaBHTjTA"
   },
   "outputs": [],
   "source": [
    "def f(X, Theta):\n",
    "    return X @ Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I15OBJ4oVG6l"
   },
   "source": [
    "2. Loss:\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{2N} (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})^\\intercal(\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyPQ37vdVQ8H"
   },
   "outputs": [],
   "source": [
    "# YHat is X @ Theta\n",
    "def J(Y, YHat):\n",
    "    N = Y.shape[0]\n",
    "    tmp = YHat-Y\n",
    "    loss = ( tmp.t() @ tmp ) / (2*N)\n",
    "    return loss.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSoo_NxwXkbg"
   },
   "source": [
    "3. Jacobian\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta_{t}) = \\frac{1}{N}\\mathbf{X}^\\intercal (f_{\\theta_t}(\\mathbf X) - \\mathbf{Y})\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_acW8PyX5y9"
   },
   "outputs": [],
   "source": [
    "def jacob(X, Y, YHat):\n",
    "    N = Y.shape[0]\n",
    "    return X.t() @ (YHat - Y)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh8xA6Wg05G9"
   },
   "source": [
    "4. Gradient Descent\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} \\leftarrow \\theta_{t} - \\eta * \\nabla_{\\theta} J(\\theta_{t}) \n",
    "\\end{equation}\n",
    "\n",
    "<!-- \\begin{equation}\n",
    "\\theta_{0} \\sim N(0,\\frac{I^{M \\times M}}{M})\n",
    "\\end{equation} -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG3u6jyC00rj"
   },
   "outputs": [],
   "source": [
    "def GD(X, Y, lr = 0.00001, nIter = 1000):\n",
    "\n",
    "    N, M = X.shape\n",
    "\n",
    "    for n in range(nIter):\n",
    "\n",
    "        if n == 0 : ThetaNow = torch.randn(M,1)/M\n",
    "        else      : ThetaNow = ThetaNext    \n",
    "\n",
    "        ThetaNext = ThetaNow - lr*jacob(X, Y, f(X, ThetaNow))\n",
    "\n",
    "    Theta = ThetaNext\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZiZ4kCXfPCR"
   },
   "source": [
    "5. Batch Gradient Descent\n",
    "\n",
    "   In practice, the size of data $ \\{(\\mathbf{x}^{(i)}, y^{(i)} )\\}_{i = 1}^{N} $ could be so large that it can't fit into a computer's memory. An alternative is to randomly sample a subset (batch) of the dataset for every interation. We call this technique as **Batch Gradient Descent**. Bigger batch size is prefered for a faster and more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G8VqLgsjwD6"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def BGD(X, Y,\n",
    "        lr = 0.00001,\n",
    "        nIter = 1000,\n",
    "        batchProportion = 0.1 # 10 percent\n",
    "        ):\n",
    "\n",
    "    N, M = X.shape\n",
    "\n",
    "    batchSize = int(N * batchProportion)\n",
    "\n",
    "    for n in range(nIter):\n",
    "\n",
    "        if n == 0 : ThetaNow = torch.randn(M,1)/M\n",
    "        else      : ThetaNow = ThetaNext\n",
    "\n",
    "        idx = np.random.choice(range(N), size=batchSize)\n",
    "\n",
    "        X_batch = X[idx]\n",
    "        Y_batch = Y[idx]\n",
    "\n",
    "        ThetaNext = ThetaNow - lr*jacob(X_batch, Y_batch, f(X_batch, ThetaNow))\n",
    "\n",
    "    Theta = ThetaNext\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLG0LNNKvX92"
   },
   "source": [
    "6. Batch Gradient descent with momentum\n",
    "\n",
    "    Graident Descent algorithm is memeryless: its weight update rule solely depends on the gradient $ \\nabla_{\\theta} J(\\theta_{t}) $ that is computed at time $t$. As a result GD tends to oscillate across non-informative slopes while only making small progress towards reaching the good local minima. \n",
    "\n",
    "    Momentum is a method that helps accelerate GD in the relevant direction and dampens oscillations. It does this by caching previous updates with a constant $\\gamma$ :\n",
    "\n",
    "    \\begin{equation}\n",
    "    v_{t+1} \\leftarrow \\gamma v_{t} + \\eta * \\nabla_{\\theta} J(\\theta_{t}) \\;\\;\\;\\;\\;\\;\\ (Step \\; 1)\n",
    "    \\end{equation}\n",
    "    \\begin{equation}\n",
    "    \\theta_{t+1} \\leftarrow \\theta_{t} -v_{t+1} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ (Step \\; 2)\n",
    "    \\end{equation}\n",
    "<!--     \\begin{equation}\n",
    "    \\theta_{0} \\sim N(0,\\frac{I^{M \\times M}}{M})\n",
    "    \\end{equation}\n",
    "    \\begin{equation}\n",
    "    v_{0} = 0^{1 \\times M}\n",
    "    \\end{equation}\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2TZrk-k3VVU"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def MBGD(X, Y,\n",
    "         lr = 0.00001,\n",
    "         gamma  = 0.9,\n",
    "         nIter = 1000,\n",
    "         batchProportion = 0.1 # 10 percent\n",
    "        ):\n",
    "\n",
    "    N, M = X.shape\n",
    "\n",
    "    batchSize = int(N * batchProportion)\n",
    "\n",
    "    for n in range(nIter):\n",
    "\n",
    "        if n == 0 : \n",
    "            ThetaNow = torch.randn(M,1)/M\n",
    "            VNow     = torch.zeros(M,1)\n",
    "        else      : \n",
    "            ThetaNow = ThetaNext\n",
    "            VNow     = VNext\n",
    "\n",
    "        idx = np.random.choice(range(N), size=batchSize)\n",
    "    \n",
    "        X_batch = X[idx]\n",
    "        Y_batch = Y[idx]\n",
    "    \n",
    "\n",
    "        VNext = gamma*VNow + lr*jacob(X_batch, Y_batch, f(X_batch, ThetaNow))    \n",
    "        ThetaNext = ThetaNow - VNext\n",
    "    \n",
    "    Theta = ThetaNext\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aa-m94VeLDt"
   },
   "source": [
    "7. closed form solver\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{Y}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP8pVTtWeGU2"
   },
   "outputs": [],
   "source": [
    "def CF(X, Y):\n",
    "    return (X.t() @ X).inverse() @ X.t() @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb6kCpy_Z3ZZ"
   },
   "source": [
    "## An Example Using the California Housing Dataset\n",
    "\n",
    "This [dataset](https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt) is a modified version of the California Housing dataset available from Luís Torgo's page (University of Porto). Luís Torgo obtained it from the StatLib repository (which is closed now). The dataset may also be downloaded from StatLib mirrors.\n",
    "\n",
    "This dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"100%\" src=\"https://www.geocurrents.info/wp-content/uploads/2016/01/California-Average-Home-Price-Map.png\"> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha_QCOOyQJq5"
   },
   "source": [
    "### Download and Process the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "VSqKbRk8rwJB",
    "outputId": "182c99c2-d76e-47d6-ad31-56e8448930b0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "mTRBW3hKtEIF",
    "outputId": "81cf6d6b-654c-48ec-d648-714682ed5bf1"
   },
   "outputs": [],
   "source": [
    "# We drop entries with missing values\n",
    "df = df.dropna(how='any')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn74CPZxuJ_q"
   },
   "source": [
    "Now lets do feature selection and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEfQHEeItgZR"
   },
   "outputs": [],
   "source": [
    "conX = df[ [\"longitude\",\n",
    "            \"latitude\",\n",
    "            \"total_rooms\",\n",
    "            \"total_bedrooms\",\n",
    "            \"population\", \n",
    "            \"households\"]].apply(lambda x: (x-x.mean()) / x.std(), axis=0)\n",
    "\n",
    "catX = pd.get_dummies( df[\"ocean_proximity\"], prefix='ocean_proximity', drop_first=True)\n",
    " \n",
    "\n",
    "X = pd.concat([conX, catX] , axis = 1)\n",
    "X[\"bias\"] = 1\n",
    "\n",
    "Y = df[[\"median_house_value\"]].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "X = torch.tensor(X.to_numpy()).type(torch.float32)\n",
    "Y = torch.tensor(Y.to_numpy()).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9yudKkzuYpu"
   },
   "source": [
    "We are all set! We have the $\\mathbf{X}$ and $\\mathbf{Y}$ which are the only inputs required for training our LR model. Lets start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPSfSIBfAsyz",
    "outputId": "87832415-ee8c-48d4-8b4c-37c51f620079"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# We use the first 80% data for training,\n",
    "# and test model performance on the remaining\n",
    "# 20% data\n",
    "\n",
    "XTrain, XTest = X[: int(len(X) * 0.8) ], X[int(len(X) * 0.8) + 1 :]\n",
    "YTrain, YTest = Y[: int(len(X) * 0.8) ], Y[int(len(X) * 0.8) + 1 :]\n",
    "\n",
    "# A random parameter\n",
    "ThetaRandom = torch.randn(XTest.shape[1], 1)\n",
    "\n",
    "print(\"training with GD ...\")\n",
    "start = time()\n",
    "ThetaGD = GD(XTrain, YTrain , lr=1e-01, nIter=1000)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "\n",
    "print(\"training with BGD ...\")\n",
    "start = time()\n",
    "ThetaBGD = BGD(XTrain, YTrain, lr=1e-01, nIter=1000, batchProportion = 0.01)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "print(\"training with MBGD ...\")\n",
    "start = time()\n",
    "ThetaBGD = MBGD(XTrain, YTrain, lr=1e-01, nIter=1000, batchProportion = 0.01)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "print(\"training with CF ...\")\n",
    "start = time()\n",
    "ThetaCF = CF(XTrain, YTrain)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c0SGaGQpIo6",
    "outputId": "67867d62-033a-4759-f073-2c0a0e4810a5"
   },
   "outputs": [],
   "source": [
    "YHatTestRandom = f(XTest, ThetaRandom)\n",
    "YHatTestGD     = f(XTest, ThetaGD)\n",
    "YHatTestBGD    = f(XTest, ThetaBGD)\n",
    "YHatTestMBGD   = f(XTest, ThetaBGD)\n",
    "YHatTestCF     = f(XTest, ThetaCF)\n",
    "\n",
    "print( f\"testLoss with random : {J(YTest,YHatTestRandom)}\" )\n",
    "print( f\"testLoss with GD     : {J(YTest,YHatTestGD)}\" )\n",
    "print( f\"testLoss with BGD    : {J(YTest,YHatTestBGD)}\" )\n",
    "print( f\"testLoss with MBGD   : {J(YTest,YHatTestMBGD)}\" )\n",
    "print( f\"testLoss with CF     : {J(YTest,YHatTestCF)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On Logistic Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scopes:\n",
    "  * Recap of logistic regression\n",
    "  * Learning with Gradient Descent\n",
    "  * Applying the above concepts to the [Breast Cancer Wisconsin (Diagnostic) Dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset) using [PyTorch](https://pytorch.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Logistic Regression\n",
    "\n",
    "Logistic regression is a binary **classification** model that fits data $\\{(\\mathbf x^{(i)}, y^{(i)})\\}$ with binary labels $y^{(i)}\\in \\{0,1\\}$. Mathematically the model with parameter $\\theta$ has the form\n",
    "\n",
    "$$\n",
    "f_\\theta(\\mathbf x) = \\sigma(\\theta^\\intercal \\mathbf x),\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the **sigmoid function** defined by\n",
    "\n",
    "$$\n",
    "\\sigma(\\xi) = \\frac{1}{1+\\exp(-\\xi)}.\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"50%\" src=\"https://miro.medium.com/max/1280/0*gKOV65tvGfY8SMem.png\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of $f_\\theta$ has range $(0,1)$.\n",
    "- $f_\\theta(\\mathbf x)$ gives the **probability** that the label $y$ for a given $\\mathbf x$ is $1$.\n",
    "- One usually use the threshold $T=0.5$ to make a binary prediction: the prediction is $0$ if $f_\\theta(\\mathbf x)\\leq 0.5$, and the prediction is $1$ if $f_\\theta(\\mathbf x)>0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regression model, one usually uses the **binary cross entropy** as the loss function\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N}\\left(-  y^{(i)}\\log( \\sigma(\\theta^{\\intercal}x^{(i)}) ) - (1-y^{(i)})\\log (1 - \\sigma(\\theta^{\\intercal}x^{(i)})) \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike the linear regression model, we do not have a close form solution for minimizer of the above loss function.\n",
    "- We can still use gradient descent to minimize $J(\\theta)$.\n",
    "- The Jacobian of the binary cross entropy is complicated. We will use PyTorch to automatically calculate derivatives for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-differentiation in PyTorch\n",
    "\n",
    "One advantage of the PyTorch package is that it can do auto-differentiation for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a tensor `a`, and tell\n",
    "# PyTorch to keep tract of differentiations\n",
    "# related to `a`.\n",
    "a = torch.tensor([1.,2.],requires_grad = True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2*a[0]+3*a[1]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward() # ask PyTorch to calculate derivatives\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor `b` has the form `2*a[0]+3*a[1]`, so the partial derivative of `b` with respect to `a[0]` is 2, and the partial derivative of `b` with respect to `a[1]` is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, gradients are accumulated.\n",
    "c = 3*a[0]+2*a[1]\n",
    "c.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient if you do not\n",
    "# intend to accumulate gradients.\n",
    "a.grad.zero_()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 3*a[0]+2*a[1]\n",
    "c.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-differentiation in PyTorch is very powerful, and it supports most build in operations in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression in Python\n",
    "\n",
    "The logistic regression model $f_\\theta(x)=\\sigma(\\theta^\\intercal x)$ is a composition of a linear model ($\\theta^\\intercal x$) and the sigmoid function ($\\sigma$). This can be easily expressed in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inSize, outSize = 31, 1\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(inSize, outSize), # linear model\n",
    "                            torch.nn.Sigmoid())               # sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instread of using hand-crafted functions, now we use build-in optimizer to perform GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entrophy loss\n",
    "J = torch.nn.BCELoss()\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "# We use Gradient Descent with Momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr = 0.01,\n",
    "                            momentum = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breast cancer wisconsin (diagnostic) dataset\n",
    "\n",
    "We use the Breast Cancer Wisconsin (Diagnostic) Dataset provided in the Python package `sklearn` to demonstrate training a logistic model. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass to describe charateristics of the cell nulcei present in the image.\n",
    "\n",
    "- 569 instances\n",
    "- dependent variable $\\mathbf x$ has 30 dimensions\n",
    "    - radius\n",
    "    - texture, smoothness\n",
    "    - perimeter, area\n",
    "    - ...\n",
    "- label $y$ has two classes\n",
    "    - $0$ for benign (357 instances)\n",
    "    - $1$ for malignant (212 instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "d = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(data = d.data, columns=d.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data, and\n",
    "# convert to torch.tensor datatype.\n",
    "df = pd.DataFrame(data = d.data, columns=d.feature_names)\n",
    "df = df.apply(lambda x: (x-x.mean()) / x.std(), axis=0)\n",
    "# df.insert(loc=0, column='bias', value=1)\n",
    "x = torch.tensor(df.to_numpy()).type(torch.float32)\n",
    "y = torch.tensor(d.target).type(torch.float32)\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us randomly split the data into training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCases = x.shape[0]\n",
    "nCases_shuffled = torch.randperm(nCases)\n",
    "train_ratio = 0.8\n",
    "train_list = nCases_shuffled[:int(train_ratio*nCases)]\n",
    "test_list = nCases_shuffled[int(train_ratio*nCases):]\n",
    "\n",
    "x_train, x_test = x[train_list], x[test_list]\n",
    "y_train, y_test = y[train_list], y[test_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performence: Pre-training\n",
    "\n",
    "Parameters in our `model` defined above are randomly initialized by PyTorch. Let us visualize how does our model with random parameters perform on the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inSize, outSize = 30, 1\n",
    "model = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "hat_y_test = model(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_pretrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_pretrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the predictions are values in the range $(0,1)$. We can see that the predictions are like random guessing. This is expected, as we have not trained our model yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use Gradient Descent to train the model for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "loss_record = []\n",
    "\n",
    "inSize, outSize = 30, 1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# binary cross entrophy loss\n",
    "J = nn.BCELoss()\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "optimizer = SGD(model.parameters(),\n",
    "                lr = 0.05,\n",
    "                momentum = 0.5)\n",
    "\n",
    "for i in range(nIter):\n",
    "    optimizer.zero_grad()\n",
    "    hat_y = model(x_train)\n",
    "    loss = J(hat_y,y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_record.append(loss.item())\n",
    "    if i%100 == 99 or i==0:\n",
    "        print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(y=loss_record,title='Loss of training')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our learned model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_y_test = model(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_aftertrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_aftertrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.add_hline(y=0.5, line_dash='dash')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the predictions are more reasonable: for malignant cases, our trained model gives high predictions (closer to $1$), and for benign cases, our trained model gives low predictions (closer to $0$). The dashed horizontal line represents $0.5$ probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Regularization\n",
    "\n",
    "We demonstrate how to do ridge and lasso logistic regression in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Logistic Regression\n",
    "\n",
    "For the ridge logistic regression, we add $L_2$ regularization to model parameters:\n",
    "\n",
    "$$\n",
    "J_{L_2}(\\theta) =  J(\\theta) + \\lambda/2 ||\\theta||_2^2,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N}\\left(-  y^{(i)}\\log( \\sigma(\\theta^{\\intercal}x^{(i)}) ) - (1-y^{(i)})\\log (1 - \\sigma(\\theta^{\\intercal}x^{(i)})) \\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "||\\theta||_2^2 = \\sum_j \\theta_j^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch the $L_2$ regularization of model parameters are supported in its build-in optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use the tensors x_train, y_train, ... defined in logistic regression section.\n",
    "\n",
    "nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "loss_record = []\n",
    "\n",
    "inSize, outSize = 30, 1\n",
    "\n",
    "model_ridge = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# binary cross entrophy loss\n",
    "J = nn.BCELoss()\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "# L2 regularization controlled by the `weight_decay` parameter.\n",
    "optimizer = SGD(model_ridge.parameters(),\n",
    "                lr = 0.05,\n",
    "                weight_decay=0.05, # L2 regularization\n",
    "                momentum = 0.5)\n",
    "\n",
    "for i in range(nIter):\n",
    "    optimizer.zero_grad()\n",
    "    hat_y = model_ridge(x_train)\n",
    "    loss = J(hat_y,y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_record.append(loss.item())\n",
    "    if i%100 == 99 or i==0:\n",
    "        print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_y_test = model_ridge(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_aftertrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_aftertrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.add_hline(y=0.5, line_dash='dash')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Logistic Regression\n",
    "\n",
    "For the LASSO logistic regression, we add $L_1$ regularization to model parameters:\n",
    "\n",
    "$$\n",
    "J_{L_1}(\\theta) = J(\\theta) + \\lambda ||\\theta||_1,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N}\\left(-  y^{(i)}\\log( \\sigma(\\theta^{\\intercal}x^{(i)}) ) - (1-y^{(i)})\\log (1 - \\sigma(\\theta^{\\intercal}x^{(i)})) \\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "||\\theta||_1 = \\sum_j |\\theta_j|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no build-in support for $L_1$ regularization in PyTorch. Let us code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use the tensors x_train, y_train, ... defined in logistic regression section.\n",
    "\n",
    "nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "loss_record = []\n",
    "\n",
    "inSize, outSize = 30, 1\n",
    "\n",
    "model_LASSO = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# binary cross entrophy loss\n",
    "J = nn.BCELoss()\n",
    "\n",
    "# L1 regularization\n",
    "lbd = 0.05\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "optimizer = SGD(model_LASSO.parameters(),\n",
    "                lr = 0.05,\n",
    "                momentum = 0.5)\n",
    "\n",
    "def L1Reg(model):\n",
    "    result = torch.tensor(0)\n",
    "    for param in model.parameters(): # iterate over all parameters of our model\n",
    "        result = result + param.abs().sum()\n",
    "\n",
    "    return result\n",
    "\n",
    "for i in range(nIter):\n",
    "    optimizer.zero_grad()\n",
    "    hat_y = model_LASSO(x_train)\n",
    "    loss = J(hat_y,y_train)\n",
    "    (loss + lbd*L1Reg(model_LASSO)).backward()\n",
    "    optimizer.step()\n",
    "    loss_record.append(loss.item())\n",
    "    if i%100 == 99 or i==0:\n",
    "        print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_y_test = model_LASSO(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_aftertrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_aftertrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.add_hline(y=0.5, line_dash='dash')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_L1 = model_LASSO[0].weight.detach().flatten()\n",
    "weight_L2 = model_ridge[0].weight.detach().flatten()\n",
    "weight = model[0].weight.detach().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Histogram(x=weight_L1, name='L1 reg.'))\n",
    "fig.add_traces(go.Histogram(x=weight_L2, name='L2 reg.'))\n",
    "fig.add_traces(go.Histogram(x=weight, name='No reg.'))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "\n",
    "What about elastic net regression, where we use a combination of the $L_1$ and the $L_2$ regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We re-use the tensors x_train, y_train, ... defined in logistic regression section.\n",
    "\n",
    "# nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "# loss_record = []\n",
    "\n",
    "# inSize, outSize = 30, 1\n",
    "\n",
    "# model_elastic = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "#                       nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# # binary cross entrophy loss\n",
    "# J = nn.BCELoss()\n",
    "\n",
    "# # L1 regularization\n",
    "# lbd = 0.025\n",
    "\n",
    "# # SGD optimizer in PyTorch\n",
    "# optimizer = SGD(params = None,\n",
    "#                 lr = 0.05,\n",
    "#                 weight_decay = 0.025,\n",
    "#                 momentum = 0.5)\n",
    "\n",
    "# def L1Reg(model):\n",
    "#     result = torch.tensor(0)\n",
    "#     for param in model.parameters(): # iterate over all parameters of our model\n",
    "#         result = result + param.abs().sum()\n",
    "\n",
    "#     return result\n",
    "\n",
    "# for i in range(nIter):\n",
    "#     optimizer.zero_grad()\n",
    "#     # Calculate prediction\n",
    "#     # Calculate loss\n",
    "#     # Loss + regularization, then calculate gradients\n",
    "#     optimizer.step()\n",
    "#     loss_record.append(loss.item())\n",
    "#     if i%100 == 99 or i==0:\n",
    "#         print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "aAUE4hqdgV5W",
    "yzmKef0IbBW2"
   ],
   "name": "Gradient_Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
