{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aAUE4hqdgV5W"
   },
   "source": [
    "# SPH6004 Hands On 1\n",
    "## Gradient Descent, Linear Regression and Logistic Regression\n",
    "\n",
    "Scopes:\n",
    "  * Recap of linear regression\n",
    "  * Loss function of a LR model\n",
    "  * Learning with Gradient Descent\n",
    "  * Applying the above techniques to the [US housing data](https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv) using [Pytorch](https://pytorch.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly==5.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the following packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8m_AfeNYBf6"
   },
   "source": [
    "# Recap of linear regression\n",
    "\n",
    "Linear Regression (LR) is one of the simplest machine learning model for predicting continuous outcomes (e.g. time, length). As shown in the figure below, LR tries to learn the statistics (patterns) that exist in a dataset by trying to draw the best fitted line to the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"75%\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model formulation\n",
    "\n",
    "Linear regression is a supervised learning model. We are given a sequence of **dependent variable** $(\\mathbf x^{(i)})_{i=1}^N$ and the corresponding **labels** $(y^{(i)})_{i=1}^N$, where each dependent variable $\\mathbf x^{(i)}$ is a $M$-dimensional vector of the form\n",
    "\n",
    "$$\n",
    "\\mathbf x^{(i)} = (x_1^{(i)}, x_2^{(i)}, \\cdots, x_M^{(i)}),\n",
    "$$\n",
    "\n",
    "and each label $y^{(i)}$ is a real number. Then the linear regression model has the form (for convenience we define $x^{(i)}_0=1$)\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{x}^{(i)}) = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{M}x_{M} = \\sum_{k = 0}^{M}{ \\theta_{k} x_{k}^{(i)} },\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\theta_i$'s are parameters of the model to be determined by fitting the model to the given data. Our goal is to find the 'best' parameters $\\hat{\\theta}_i$ such that $f_{\\hat{\\theta}}(\\mathbf x^{(i)})\\sim y^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We find $\\hat{\\theta}$ by minimizing the **Mean Square Error** $ J(\\theta) $ between $f_{\\theta}(\\mathbf{x})$ and $y$:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\frac{1}{2N} \\sum_{i = 1}^{N} \\left( f_\\theta(\\mathbf{x}^{(i)}) - y^{(i)} \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "This function $J(\\theta)$ is called a **loss function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To matrix form\n",
    "\n",
    "It is often conventient to re-respresent math equations that deals with large amount of data as concise matrix operations. Therefore, the equations above can be re-written as (recall we have $\\mathbf{x}^{(i)}=(1,x^{(i)}_1,\\cdots, x^{(i)}_M)$):\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{x}^{(i)}) = \\mathbf{\\theta}^\\intercal \\mathbf{x}^{(i)},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\frac{1}{2N} (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})^\\intercal (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{X}\\in \\mathbb{R}^{N \\times (M+1)} $ and $\\mathbf{Y} \\in \\mathbb{R}^{N \\times 1}$ are matrix representations of training input variables  $\\{{\\mathbf{x}^{(i)}}\\}_{i = 1}^{N}$ and prediction variables $\\{{y^{(i)}}\\}_{i = 1}^{N}$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzmKef0IbBW2"
   },
   "source": [
    "\n",
    "## Finding the best $\\hat{\\theta}$\n",
    "\n",
    "We minimize the loss function to obtain the best $\\hat{\\theta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = \\arg\\min_{\\theta} J(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Here we introduce two ways to minimize the fuction: 1) by Gradient Descent: a first-order iterative optimization algorithm; 2) by solving the Loss function's first-order derivative equation. \n",
    "\n",
    "\n",
    "<table><tr>\n",
    "\n",
    "<td> <img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" alt=\"Drawing\" style= width=\"200\" height=\"200\"/> </td>\n",
    "\n",
    "<td> <img src=\"https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png\" alt=\"Drawing\" stylewidth=\"200\" height=\"200\"/> </td>\n",
    "\n",
    "</tr></table>\n",
    "\n",
    "### 1) Gradient Descent\n",
    "\n",
    "Gradient Descent finds the minimum of a loss function by repeatly taking steps in the opposite direction of the gradient of the function, because this is the direction of steepest descent. Overtime, the weights of the model navigates to a point where the loss is very close to its minimum.\n",
    "\n",
    "Formally, given a parameter $\\theta_{t}$ at time $t$, we compute the next parameter $\\theta_{t+1}$ as :\n",
    "\n",
    "\\begin{equation} \n",
    "\\theta_{t+1} \\leftarrow \\theta_{t} - \\eta * \\nabla_{\\theta} J(\\theta_{t})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta$ is a learning rate usually set within $[0.00001, 0.1]$ and $\\nabla_{\\theta} J(\\theta_{t}) $ is the Jacobian of the loss function\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J = [\\frac{\\partial J(\\theta)}{\\partial \\theta_0}, \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, ..., \\frac{\\partial J(\\theta)}{\\partial \\theta_M}]^\\intercal = \\frac{1}{N}\\mathbf X^\\intercal (\\mathbf X\\theta - \\mathbf Y)\n",
    "\\end{equation}\n",
    "\n",
    "evaluated at $\\theta_t$.\n",
    "\n",
    "### 2) Solving the Loss function's first-order derivative equation\n",
    "\n",
    "One advantage of linear regression is that its loss function's first-order derivative equation is solvable. Using the differentiation rules, we can easily get the function's first-order derivative equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_i \\mathbf{x}^{(i)}_k \\left(f_{\\hat{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)}\\right) = 0,\\quad k=0,1,\\cdots, M.\n",
    "\\end{equation}\n",
    "\n",
    "Solving these first-order equations gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^\\intercal \\mathbf{X})^{-1}\\mathbf{X}^\\intercal \\mathbf{Y}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmHvNO6Mpe8C"
   },
   "source": [
    "-----\n",
    "-----\n",
    "# Linear Regression in Python\n",
    "\n",
    "For this tutorial, we will use Python with Pytorch API to implement linear regression. [Pytorch](https://pytorch.org/) is a Deep learning framework developed by facebook to accelerate Deep learning research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1zn0hUwK4UZ"
   },
   "source": [
    "## Basic math operations in Pytorch\n",
    "\n",
    "Pytorch provides many useful basic matrix operations such as multiplication, transpose, inversion etc. **To use these operations we first need to  declare our data as a tensor datatype**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1IZlHgGM8QG"
   },
   "outputs": [],
   "source": [
    "# call a package\n",
    "import torch \n",
    "\n",
    "# generate a random tensor\n",
    "X1 = torch.randn(2,2)\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pointwise multiplication\n",
    "O0 = X1*X1\n",
    "print(O0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose\n",
    "O1 = X1.t()\n",
    "print(O1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "O2 = X1 @ torch.transpose(X1, 1, 0) \n",
    "print(O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse\n",
    "O3 = X1.inverse()\n",
    "print(O3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iblDFoPQ5ax"
   },
   "source": [
    "--------\n",
    "Now, lets convert maths to codes\n",
    "\n",
    "1. Inference :\n",
    "\\begin{equation}\n",
    "f_{\\theta}(\\mathbf{X}) = f( \\mathbf{X} ;\\theta )  = \\mathbf{X}\\mathbf{\\theta}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1kGSaBHTjTA"
   },
   "outputs": [],
   "source": [
    "def f(X, Theta):\n",
    "    return X @ Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I15OBJ4oVG6l"
   },
   "source": [
    "2. Loss:\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{2N} (\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})^\\intercal(\\mathbf{X}\\mathbf{\\theta} - \\mathbf{Y})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyPQ37vdVQ8H"
   },
   "outputs": [],
   "source": [
    "# YHat is X @ Theta\n",
    "def J(Y, YHat):\n",
    "    N = Y.shape[0]\n",
    "    tmp = YHat-Y\n",
    "    loss = ( tmp.t() @ tmp ) / (2*N)\n",
    "    return loss.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSoo_NxwXkbg"
   },
   "source": [
    "3. Jacobian\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta_{t}) = \\frac{1}{N}\\mathbf{X}^\\intercal (f_{\\theta_t}(\\mathbf X) - \\mathbf{Y})\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_acW8PyX5y9"
   },
   "outputs": [],
   "source": [
    "def jacob(X, Y, YHat):\n",
    "    N = Y.shape[0]\n",
    "    return X.t() @ (YHat - Y)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh8xA6Wg05G9"
   },
   "source": [
    "4. Gradient Descent\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} \\leftarrow \\theta_{t} - \\eta * \\nabla_{\\theta} J(\\theta_{t}) \n",
    "\\end{equation}\n",
    "\n",
    "<!-- \\begin{equation}\n",
    "\\theta_{0} \\sim N(0,\\frac{I^{M \\times M}}{M})\n",
    "\\end{equation} -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG3u6jyC00rj"
   },
   "outputs": [],
   "source": [
    "def GD(X, Y, lr = 0.00001, nIter = 1000):\n",
    "\n",
    "    N, M = X.shape\n",
    "\n",
    "    for n in range(nIter):\n",
    "\n",
    "        if n == 0 : ThetaNow = torch.randn(M,1)/M\n",
    "        else      : ThetaNow = ThetaNext    \n",
    "\n",
    "        ThetaNext = ThetaNow - lr*jacob(X, Y, f(X, ThetaNow))\n",
    "\n",
    "    Theta = ThetaNext\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZiZ4kCXfPCR"
   },
   "source": [
    "5. Batch Gradient Descent\n",
    "\n",
    "   In practice, the size of data $ \\{(\\mathbf{x}^{(i)}, y^{(i)} )\\}_{i = 1}^{N} $ could be so large that it can't fit into a computer's memory. An alternative is to randomly sample a subset (batch) of the dataset for every interation. We call this technique as **Batch Gradient Descent**. Bigger batch size is prefered for a faster and more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G8VqLgsjwD6"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def BGD(X, Y,\n",
    "        lr = 0.00001,\n",
    "        nIter = 1000,\n",
    "        batchProportion = 0.1 # 10 percent\n",
    "        ):\n",
    "\n",
    "    N, M = X.shape\n",
    "\n",
    "    batchSize = int(N * batchProportion)\n",
    "\n",
    "    for n in range(nIter):\n",
    "\n",
    "        if n == 0 : ThetaNow = torch.randn(M,1)/M\n",
    "        else      : ThetaNow = ThetaNext\n",
    "\n",
    "        idx = np.random.choice(range(N), size=batchSize)\n",
    "\n",
    "        X_batch = X[idx]\n",
    "        Y_batch = Y[idx]\n",
    "\n",
    "        ThetaNext = ThetaNow - lr*jacob(X_batch, Y_batch, f(X_batch, ThetaNow))\n",
    "\n",
    "    Theta = ThetaNext\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YLG0LNNKvX92"
   },
   "source": [
    "6. Batch Gradient descent with momentum\n",
    "\n",
    "    Graident Descent algorithm is memeryless: its weight update rule solely depends on the gradient $ \\nabla_{\\theta} J(\\theta_{t}) $ that is computed at time $t$. As a result GD tends to oscillate across non-informative slopes while only making small progress towards reaching the good local minima. \n",
    "\n",
    "    Momentum is a method that helps accelerate GD in the relevant direction and dampens oscillations. It does this by caching previous updates with a constant $\\gamma$ :\n",
    "\n",
    "    \\begin{equation}\n",
    "    v_{t+1} \\leftarrow \\gamma v_{t} + \\eta * \\nabla_{\\theta} J(\\theta_{t}) \\;\\;\\;\\;\\;\\;\\ (Step \\; 1)\n",
    "    \\end{equation}\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\theta_{t+1} \\leftarrow \\theta_{t} -v_{t+1} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ (Step \\; 2)\n",
    "    \\end{equation}\n",
    "<!--     \\begin{equation}\n",
    "    \\theta_{0} \\sim N(0,\\frac{I^{M \\times M}}{M})\n",
    "    \\end{equation}\n",
    "    \\begin{equation}\n",
    "    v_{0} = 0^{1 \\times M}\n",
    "    \\end{equation}\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2TZrk-k3VVU"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def MBGD(X, Y,\n",
    "         lr = 0.00001,\n",
    "         gamma  = 0.9,\n",
    "         nIter = 1000,\n",
    "         batchProportion = 0.1 # 10 percent\n",
    "        ):\n",
    "\n",
    "    N, M = X.shape\n",
    "\n",
    "    batchSize = int(N * batchProportion)\n",
    "\n",
    "    for n in range(nIter):\n",
    "\n",
    "        if n == 0 : \n",
    "            ThetaNow = torch.randn(M,1)/M\n",
    "            VNow     = torch.zeros(M,1)\n",
    "        else      : \n",
    "            ThetaNow = ThetaNext\n",
    "            VNow     = VNext\n",
    "\n",
    "        idx = np.random.choice(range(N), size=batchSize)\n",
    "    \n",
    "        X_batch = X[idx]\n",
    "        Y_batch = Y[idx]\n",
    "    \n",
    "\n",
    "        VNext = gamma*VNow + lr*jacob(X_batch, Y_batch, f(X_batch, ThetaNow))    \n",
    "        ThetaNext = ThetaNow - VNext\n",
    "    \n",
    "    Theta = ThetaNext\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aa-m94VeLDt"
   },
   "source": [
    "7. closed form solver\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{Y}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP8pVTtWeGU2"
   },
   "outputs": [],
   "source": [
    "def CF(X, Y):\n",
    "    return (X.t() @ X).inverse() @ X.t() @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb6kCpy_Z3ZZ"
   },
   "source": [
    "## An Example Using the California Housing Dataset\n",
    "\n",
    "This [dataset](https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt) is a modified version of the California Housing dataset available from Luís Torgo's page (University of Porto). Luís Torgo obtained it from the StatLib repository (which is closed now). The dataset may also be downloaded from StatLib mirrors.\n",
    "\n",
    "This dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"100%\" src=\"https://www.geocurrents.info/wp-content/uploads/2016/01/California-Average-Home-Price-Map.png\"> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha_QCOOyQJq5"
   },
   "source": [
    "### Download and Process the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "VSqKbRk8rwJB",
    "outputId": "182c99c2-d76e-47d6-ad31-56e8448930b0"
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\urllib\\request.py:1346\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1346\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[0;32m   1347\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m   1348\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\http\\client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mConnect to a host on a given (SSL) port.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1447\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\http\\client.py:946\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[39m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(\n\u001b[0;32m    947\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address)\n\u001b[0;32m    948\u001b[0m \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\socket.py:844\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 844\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    845\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    846\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\socket.py:832\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 832\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m    833\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hfyang\\OneDrive - National University of Singapore\\Lectures\\SPH6004\\Advanced-Biostatistics-Hands-On\\Hands_On_1.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hfyang/OneDrive%20-%20National%20University%20of%20Singapore/Lectures/SPH6004/Advanced-Biostatistics-Hands-On/Hands_On_1.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hfyang/OneDrive%20-%20National%20University%20of%20Singapore/Lectures/SPH6004/Advanced-Biostatistics-Hands-On/Hands_On_1.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hfyang/OneDrive%20-%20National%20University%20of%20Singapore/Lectures/SPH6004/Advanced-Biostatistics-Hands-On/Hands_On_1.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(url)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hfyang/OneDrive%20-%20National%20University%20of%20Singapore/Lectures/SPH6004/Advanced-Biostatistics-Hands-On/Hands_On_1.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:670\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    667\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    669\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[1;32m--> 670\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    671\u001b[0m     path_or_buf,\n\u001b[0;32m    672\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    673\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    674\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    675\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    676\u001b[0m )\n\u001b[0;32m    678\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    679\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:339\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    338\u001b[0m req_info \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[39m=\u001b[39mstorage_options)\n\u001b[1;32m--> 339\u001b[0m \u001b[39mwith\u001b[39;00m urlopen(req_info) \u001b[39mas\u001b[39;00m req:\n\u001b[0;32m    340\u001b[0m     content_encoding \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Encoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    341\u001b[0m     \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    342\u001b[0m         \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:239\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39mthe stdlib.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\urllib\\request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    516\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 517\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    519\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    520\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\urllib\\request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    533\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 534\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    535\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\urllib\\request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1390\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[1;32mc:\\Users\\hfyang\\Anaconda3\\lib\\urllib\\request.py:1349\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[0;32m   1347\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1350\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m   1351\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "mTRBW3hKtEIF",
    "outputId": "81cf6d6b-654c-48ec-d648-714682ed5bf1"
   },
   "outputs": [],
   "source": [
    "# We drop entries with missing values\n",
    "df = df.dropna(how='any')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn74CPZxuJ_q"
   },
   "source": [
    "Now lets do feature selection and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEfQHEeItgZR"
   },
   "outputs": [],
   "source": [
    "conX = df[ [\"longitude\",\n",
    "            \"latitude\",\n",
    "            \"total_rooms\",\n",
    "            \"total_bedrooms\",\n",
    "            \"population\", \n",
    "            \"households\"]].apply(lambda x: (x-x.mean()) / x.std(), axis=0)\n",
    "\n",
    "catX = pd.get_dummies( df[\"ocean_proximity\"], prefix='ocean_proximity', drop_first=True)\n",
    " \n",
    "\n",
    "X = pd.concat([conX, catX] , axis = 1)\n",
    "X[\"bias\"] = 1\n",
    "\n",
    "Y = df[[\"median_house_value\"]].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "X = torch.tensor(X.to_numpy()).type(torch.float32)\n",
    "Y = torch.tensor(Y.to_numpy()).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9yudKkzuYpu"
   },
   "source": [
    "We are all set! We have the $\\mathbf{X}$ and $\\mathbf{Y}$ which are the only inputs required for training our LR model. Lets start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPSfSIBfAsyz",
    "outputId": "87832415-ee8c-48d4-8b4c-37c51f620079"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# We use the first 80% data for training,\n",
    "# and test model performance on the remaining\n",
    "# 20% data\n",
    "\n",
    "XTrain, XTest = X[: int(len(X) * 0.8) ], X[int(len(X) * 0.8) + 1 :]\n",
    "YTrain, YTest = Y[: int(len(X) * 0.8) ], Y[int(len(X) * 0.8) + 1 :]\n",
    "\n",
    "# A random parameter\n",
    "ThetaRandom = torch.randn(XTest.shape[1], 1)\n",
    "\n",
    "print(\"training with GD ...\")\n",
    "start = time()\n",
    "ThetaGD = GD(XTrain, YTrain , lr=1e-01, nIter=1000)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "\n",
    "print(\"training with BGD ...\")\n",
    "start = time()\n",
    "ThetaBGD = BGD(XTrain, YTrain, lr=1e-01, nIter=1000, batchProportion = 0.01)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "print(\"training with MBGD ...\")\n",
    "start = time()\n",
    "ThetaBGD = MBGD(XTrain, YTrain, lr=1e-01, nIter=1000, batchProportion = 0.01)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n",
    "print(\"training with CF ...\")\n",
    "start = time()\n",
    "ThetaCF = CF(XTrain, YTrain)\n",
    "end = time()\n",
    "print(f\"took { str(end - start)[:7] } seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c0SGaGQpIo6",
    "outputId": "67867d62-033a-4759-f073-2c0a0e4810a5"
   },
   "outputs": [],
   "source": [
    "YHatTestRandom = f(XTest, ThetaRandom)\n",
    "YHatTestGD     = f(XTest, ThetaGD)\n",
    "YHatTestBGD    = f(XTest, ThetaBGD)\n",
    "YHatTestMBGD   = f(XTest, ThetaBGD)\n",
    "YHatTestCF     = f(XTest, ThetaCF)\n",
    "\n",
    "print( f\"testLoss with random : {J(YTest,YHatTestRandom)}\" )\n",
    "print( f\"testLoss with GD     : {J(YTest,YHatTestGD)}\" )\n",
    "print( f\"testLoss with BGD    : {J(YTest,YHatTestBGD)}\" )\n",
    "print( f\"testLoss with MBGD   : {J(YTest,YHatTestMBGD)}\" )\n",
    "print( f\"testLoss with CF     : {J(YTest,YHatTestCF)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On Logistic Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scopes:\n",
    "  * Recap of logistic regression\n",
    "  * Learning with Gradient Descent\n",
    "  * Applying the above concepts to the [Breast Cancer Wisconsin (Diagnostic) Dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset) using [PyTorch](https://pytorch.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Logistic Regression\n",
    "\n",
    "Logistic regression is a binary **classification** model that fits data $\\{(\\mathbf x^{(i)}, y^{(i)})\\}$ with binary labels $y^{(i)}\\in \\{0,1\\}$. Mathematically the model with parameter $\\theta$ has the form\n",
    "\n",
    "$$\n",
    "f_\\theta(\\mathbf x) = \\sigma(\\theta^\\intercal \\mathbf x),\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the **sigmoid function** defined by\n",
    "\n",
    "$$\n",
    "\\sigma(\\xi) = \\frac{1}{1+\\exp(-\\xi)}.\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"50%\" src=\"https://miro.medium.com/max/1280/0*gKOV65tvGfY8SMem.png\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of $f_\\theta$ has range $(0,1)$.\n",
    "- $f_\\theta(\\mathbf x)$ gives the **probability** that the label $y$ for a given $\\mathbf x$ is $1$.\n",
    "- One usually use the threshold $T=0.5$ to make a binary prediction: the prediction is $0$ if $f_\\theta(\\mathbf x)\\leq 0.5$, and the prediction is $1$ if $f_\\theta(\\mathbf x)>0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regression model, one usually uses the **binary cross entropy** as the loss function\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N}\\left(-  y^{(i)}\\log( \\sigma(\\theta^{\\intercal}x^{(i)}) ) - (1-y^{(i)})\\log (1 - \\sigma(\\theta^{\\intercal}x^{(i)})) \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike the linear regression model, we do not have a close form solution for minimizer of the above loss function.\n",
    "- We can still use gradient descent to minimize $J(\\theta)$.\n",
    "- The Jacobian of the binary cross entropy is complicated. We will use PyTorch to automatically calculate derivatives for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-differentiation in PyTorch\n",
    "\n",
    "One advantage of the PyTorch package is that it can do auto-differentiation for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a tensor `a`, and tell\n",
    "# PyTorch to keep tract of differentiations\n",
    "# related to `a`.\n",
    "a = torch.tensor([1.,2.],requires_grad = True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2*a[0]+3*a[1]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward() # ask PyTorch to calculate derivatives\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor `b` has the form `2*a[0]+3*a[1]`, so the partial derivative of `b` with respect to `a[0]` is 2, and the partial derivative of `b` with respect to `a[1]` is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, gradients are accumulated.\n",
    "c = 3*a[0]+2*a[1]\n",
    "c.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient if you do not\n",
    "# intend to accumulate gradients.\n",
    "a.grad.zero_()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 3*a[0]+2*a[1]\n",
    "c.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-differentiation in PyTorch is very powerful, and it supports most build in operations in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression in Python\n",
    "\n",
    "The logistic regression model $f_\\theta(x)=\\sigma(\\theta^\\intercal x)$ is a composition of a linear model ($\\theta^\\intercal x$) and the sigmoid function ($\\sigma$). This can be easily expressed in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inSize, outSize = 31, 1\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(inSize, outSize), # linear model\n",
    "                            torch.nn.Sigmoid())               # sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instread of using hand-crafted functions, now we use build-in optimizer to perform GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entrophy loss\n",
    "J = torch.nn.BCELoss()\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "# We use Gradient Descent with Momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr = 0.01,\n",
    "                            momentum = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breast cancer wisconsin (diagnostic) dataset\n",
    "\n",
    "We use the Breast Cancer Wisconsin (Diagnostic) Dataset provided in the Python package `sklearn` to demonstrate training a logistic model. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass to describe charateristics of the cell nulcei present in the image.\n",
    "\n",
    "- 569 instances\n",
    "- dependent variable $\\mathbf x$ has 30 dimensions\n",
    "    - radius\n",
    "    - texture, smoothness\n",
    "    - perimeter, area\n",
    "    - ...\n",
    "- label $y$ has two classes\n",
    "    - $0$ for benign (357 instances)\n",
    "    - $1$ for malignant (212 instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "d = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(data = d.data, columns=d.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data, and\n",
    "# convert to torch.tensor datatype.\n",
    "df = pd.DataFrame(data = d.data, columns=d.feature_names)\n",
    "df = df.apply(lambda x: (x-x.mean()) / x.std(), axis=0)\n",
    "# df.insert(loc=0, column='bias', value=1)\n",
    "x = torch.tensor(df.to_numpy()).type(torch.float32)\n",
    "y = torch.tensor(d.target).type(torch.float32)\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us randomly split the data into training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCases = x.shape[0]\n",
    "nCases_shuffled = torch.randperm(nCases)\n",
    "train_ratio = 0.8\n",
    "train_list = nCases_shuffled[:int(train_ratio*nCases)]\n",
    "test_list = nCases_shuffled[int(train_ratio*nCases):]\n",
    "\n",
    "x_train, x_test = x[train_list], x[test_list]\n",
    "y_train, y_test = y[train_list], y[test_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performence: Pre-training\n",
    "\n",
    "Parameters in our `model` defined above are randomly initialized by PyTorch. Let us visualize how does our model with random parameters perform on the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inSize, outSize = 30, 1\n",
    "model = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "hat_y_test = model(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_pretrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_pretrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the predictions are values in the range $(0,1)$. We can see that the predictions are like random guessing. This is expected, as we have not trained our model yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use Gradient Descent to train the model for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "loss_record = []\n",
    "\n",
    "inSize, outSize = 30, 1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# binary cross entrophy loss\n",
    "J = nn.BCELoss()\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "optimizer = SGD(model.parameters(),\n",
    "                lr = 0.05,\n",
    "                momentum = 0.5)\n",
    "\n",
    "for i in range(nIter):\n",
    "    optimizer.zero_grad()\n",
    "    hat_y = model(x_train)\n",
    "    loss = J(hat_y,y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_record.append(loss.item())\n",
    "    if i%100 == 99 or i==0:\n",
    "        print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(y=loss_record,title='Loss of training')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our learned model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_y_test = model(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_aftertrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_aftertrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.add_hline(y=0.5, line_dash='dash')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the predictions are more reasonable: for malignant cases, our trained model gives high predictions (closer to $1$), and for benign cases, our trained model gives low predictions (closer to $0$). The dashed horizontal line represents $0.5$ probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Regularization\n",
    "\n",
    "We demonstrate how to do ridge and lasso logistic regression in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Logistic Regression\n",
    "\n",
    "For the ridge logistic regression, we add $L_2$ regularization to model parameters:\n",
    "\n",
    "$$\n",
    "J_{L_2}(\\theta) =  J(\\theta) + \\lambda/2 ||\\theta||_2^2,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N}\\left(-  y^{(i)}\\log( \\sigma(\\theta^{\\intercal}x^{(i)}) ) - (1-y^{(i)})\\log (1 - \\sigma(\\theta^{\\intercal}x^{(i)})) \\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "||\\theta||_2^2 = \\sum_j \\theta_j^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch the $L_2$ regularization of model parameters are supported in its build-in optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use the tensors x_train, y_train, ... defined in logistic regression section.\n",
    "\n",
    "nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "loss_record = []\n",
    "\n",
    "inSize, outSize = 30, 1\n",
    "\n",
    "model_ridge = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# binary cross entrophy loss\n",
    "J = nn.BCELoss()\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "# L2 regularization controlled by the `weight_decay` parameter.\n",
    "optimizer = SGD(model_ridge.parameters(),\n",
    "                lr = 0.05,\n",
    "                weight_decay=0.05, # L2 regularization\n",
    "                momentum = 0.5)\n",
    "\n",
    "for i in range(nIter):\n",
    "    optimizer.zero_grad()\n",
    "    hat_y = model_ridge(x_train)\n",
    "    loss = J(hat_y,y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_record.append(loss.item())\n",
    "    if i%100 == 99 or i==0:\n",
    "        print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_y_test = model_ridge(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_aftertrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_aftertrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.add_hline(y=0.5, line_dash='dash')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Logistic Regression\n",
    "\n",
    "For the LASSO logistic regression, we add $L_1$ regularization to model parameters:\n",
    "\n",
    "$$\n",
    "J_{L_1}(\\theta) = J(\\theta) + \\lambda ||\\theta||_1,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N}\\left(-  y^{(i)}\\log( \\sigma(\\theta^{\\intercal}x^{(i)}) ) - (1-y^{(i)})\\log (1 - \\sigma(\\theta^{\\intercal}x^{(i)})) \\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "||\\theta||_1 = \\sum_j |\\theta_j|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no build-in support for $L_1$ regularization in PyTorch. Let us code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use the tensors x_train, y_train, ... defined in logistic regression section.\n",
    "\n",
    "nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "loss_record = []\n",
    "\n",
    "inSize, outSize = 30, 1\n",
    "\n",
    "model_LASSO = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "                      nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# binary cross entrophy loss\n",
    "J = nn.BCELoss()\n",
    "\n",
    "# L1 regularization\n",
    "lbd = 0.05\n",
    "\n",
    "# SGD optimizer in PyTorch\n",
    "optimizer = SGD(model_LASSO.parameters(),\n",
    "                lr = 0.05,\n",
    "                momentum = 0.5)\n",
    "\n",
    "def L1Reg(model):\n",
    "    result = torch.tensor(0)\n",
    "    for param in model.parameters(): # iterate over all parameters of our model\n",
    "        result = result + param.abs().sum()\n",
    "\n",
    "    return result\n",
    "\n",
    "for i in range(nIter):\n",
    "    optimizer.zero_grad()\n",
    "    hat_y = model_LASSO(x_train)\n",
    "    loss = J(hat_y,y_train)\n",
    "    (loss + lbd*L1Reg(model_LASSO)).backward()\n",
    "    optimizer.step()\n",
    "    loss_record.append(loss.item())\n",
    "    if i%100 == 99 or i==0:\n",
    "        print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_y_test = model_LASSO(x_test).detach()\n",
    "label = y_test.squeeze().type(torch.bool)\n",
    "label = ['malignant' if tmp else 'benign' for tmp in label]\n",
    "\n",
    "df_aftertrain = pd.DataFrame({'prediction':hat_y_test.squeeze(),\n",
    "                            'label':label\n",
    "                           }\n",
    "                          )\n",
    "\n",
    "fig = px.scatter(data_frame=df_aftertrain,\n",
    "                 y='prediction',\n",
    "                 color='label',\n",
    "                 labels={True:'malignant',\n",
    "                         'benign':False})\n",
    "fig.add_hline(y=0.5, line_dash='dash')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_L1 = model_LASSO[0].weight.detach().flatten()\n",
    "weight_L2 = model_ridge[0].weight.detach().flatten()\n",
    "weight = model[0].weight.detach().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Histogram(x=weight_L1, name='L1 reg.'))\n",
    "fig.add_traces(go.Histogram(x=weight_L2, name='L2 reg.'))\n",
    "fig.add_traces(go.Histogram(x=weight, name='No reg.'))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "\n",
    "What about elastic net regression, where we use a combination of the $L_1$ and the $L_2$ regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We re-use the tensors x_train, y_train, ... defined in logistic regression section.\n",
    "\n",
    "# nIter = 1000 # We perform 1000 iterations of GD steps\n",
    "# loss_record = []\n",
    "\n",
    "# inSize, outSize = 30, 1\n",
    "\n",
    "# model_elastic = nn.Sequential(nn.Linear(inSize, outSize), # innner product\n",
    "#                       nn.Sigmoid())               # sigmoid\n",
    "\n",
    "# # binary cross entrophy loss\n",
    "# J = nn.BCELoss()\n",
    "\n",
    "# # L1 regularization\n",
    "# lbd = 0.025\n",
    "\n",
    "# # SGD optimizer in PyTorch\n",
    "# optimizer = SGD(params = None,\n",
    "#                 lr = 0.05,\n",
    "#                 weight_decay = 0.025,\n",
    "#                 momentum = 0.5)\n",
    "\n",
    "# def L1Reg(model):\n",
    "#     result = torch.tensor(0)\n",
    "#     for param in model.parameters(): # iterate over all parameters of our model\n",
    "#         result = result + param.abs().sum()\n",
    "\n",
    "#     return result\n",
    "\n",
    "# for i in range(nIter):\n",
    "#     optimizer.zero_grad()\n",
    "#     # Calculate prediction\n",
    "#     # Calculate loss\n",
    "#     # Loss + regularization, then calculate gradients\n",
    "#     optimizer.step()\n",
    "#     loss_record.append(loss.item())\n",
    "#     if i%100 == 99 or i==0:\n",
    "#         print('At iteration {} loss is {:.4f}'.format(i+1,loss.item()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "aAUE4hqdgV5W",
    "yzmKef0IbBW2"
   ],
   "name": "Gradient_Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2d1f8e1ab2e6d576e713f7bdd1284bcd61f3d0375efe1472777f64149d9b990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
